Welcome to Low-Rank Adaptation or LoRA. After watching this video, you'll be able to define LoRA and explain its working. LoRA simplifies the large and complex machine learning model suitable for specific uses. LoRA works by adding lightweight plug-ins to the original model for efficient functioning. LoRA enables reduced trainable parameters by leveraging pre-trained models with high dimensional matrices, such as the network's pre-trained parameters. LoRA decreases the training time, resource usage, and memory footprint. Let's understand this with an example. Consider the third layer of the network displayed on the screen, with the input directions of ten and eight neurons, which equals 80 parameters. Now, consider a layer with an input dimension of ten and three neurons, which leads to 30 parameters. Next, map the three dimensional output to eight neurons with three dimensions, each with three parameters, leading to 24 parameters. This approach reduces the number of trainable parameters to 54 to maintain the model performance. It is interesting to note that LoRA, operates in the same way. However, bias is not included for simplicity. Let's understand LoRA using the matrix algebra. The original neural network layer has a weight matrix w zero with dimensions d by k, where d is the input size, and k is the output size. The layers output gets computed as a function of h(x)=W_0x, x is the input from the last layer. Hence, the result in parameter is d * k. In matrix algebra, to remain consistent with the original LoRa layer, let's use x and not be confused with the feature. Next, LoRA, while predicting the forward step, an additional matrix delta W is added to the original weight matrix W_0. Therefore, the new weight matrix is W=W_0 plus delta W, and the layers output becomes a function h of x, which equals the quantity W_0 plus delta W(x). Next, upon decomposing the LoRa update, the matrix delta W decomposes into two low rank matrices, B and A, meaning delta W=B * A. A and B are the additional layers that map from a simpler to a larger dimension. The dimensions of B are D x r, and that of A is R x R is the rank. The value of rank r should be smaller than d and k. Let's understand this with an example. Consider that r is a hyper parameter, and in LoRa, delta W is a product of B and A matrices. Now upon setting r=1, the resultant matrices B and A have just one dimension, but the product size remains fixed. The increase in r increases the number of parameters and keeps higher model capacity, keeping the product size constant. Next, upon fine tuning with LoRA, the original weight matrix W_0 remains frozen and isn't updated during training. This approach significantly reduces the number of learnable parameters. In the original neural network, the number of learnable parameters is d * k. And with LoRA, the number of learnable parameters reduces to d * r + r * k, which is much smaller than d * k. This reduction occurs because the training updates only the low rank matrices, B and A, instead of the full weight matrix W. Hence, this method saves computation and memory during the backward pass. Let's understand how to optimize LoRA. In the forward step of LoRa, apply scaling factor by multiplying Alpha divided by the rank r, both being hyperparameters. The displayed loss function involves x as the input sequence and y as the target. The loss function depends on matrices A and B. Meaning only the delta W is updated using a gradient descent algorithm, typically Adam. LoRA is a general framework for optimizing parameters, and it is useful for transformers having multiple parameters, specifically in the attention layers. Moreover, it optimizes the key query and value parameters represented by delta theta. It means that loss function, delta theta can apply to encoders and decoders. It does not necessarily have to be cross entropy loss. Following the notation from the original LoRA paper, The log likelihood acts as a function of the LoRa parameters. Therefore, the summation over t is for each prediction in the auto regressive sequence, and the second summation is over each sample. Here, omega represents the tokens, and x represents the corresponding embeddings. Let's recap. In this video, you learned about LoRA and its working. LoRA helps complex machine learning for specific uses by adding lightweight plug-in components to the original model. It reduces the number of trainable parameters by leveraging pre-trained models and matrix algebra to decompose weight updates into low rank matrices. During the forward pass, LoRA adds an extra matrix delta W to the original weight matrix, enabling computation with fewer parameters by maintaining model performance. The original weight matrix remains frozen in the fine tuning process, and only low rank matrices are updated. Lastly, LoRA is applicable for transformers, optimizing key, query, and value parameters in attention layers, and applied to encoders and decoders to save computation and memory during the training and storage.