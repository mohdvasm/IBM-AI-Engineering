
[MUSIC] Welcome to Fine-tuning with Hugging Face. After watching this video, you'll be able to describe how to fine-tune your model with Hugging Face and PyTorch. You'll also be able to use the supervised fine-tuning trainer, or SFT Trainer, to fine-tune your model. Hugging Face is an open-source machine learning, or ML, platform with a built-in transformers library for natural language processing, or NLP, applications. The platform allows users to share machine learning models and datasets and showcase their work. Hugging Face's built-in datasets can be loaded using the load_dataset function. Let's load the Yelp review dataset which is a list like object consisting of user reviews and accompanying metadata from the Yelp platform. Each review is a dictionary typically containing the text of the review itself with the key text and the star rating given by the user which ranges from one to five with the key label. You can load a BERTtokenizer object to tokenize pad and truncate reviews which helps handle variable length sequences efficiently. The tokenizer function extracts the text from the dataset example and applies the tokenizer. You can then map this method to the dataset. The result is that each sample text in the dataset has been tokenized, converting the text to token indices with the dictionary key input ids. Additionally, other parameters associated with the BERT model such as attention masks have been generated for each sample. As the model does not need text information, you can remove the text and rename the label key. The data is then converted into PyTorch tensors. The result is a set of tensors with the keys, labels, input ids, token type ids, and attention mask. Just like PyTorch, in Hugging Face, you can create a data loader object for train and test datasets. This allows you to iterate over batches. Now you are going to load a pre-trained BERT classification model with five classes from the transformer's library. The module is designed for sequence classification. The num labels parameter specifies the numbers of classes and determines the number of neurons in the final layer.
Play video starting at :2:19 and follow transcript2:19
Lets create an optimizer and learning rate scheduler to fine-tune the model. You can use the AdamW optimizer from PyTorch and set the device accordingly. Now let's create a training function to fine-tune the model. Here the evaluation function has been defined to evaluate the model's performance after fine-tuning it. You can now train the model and observe the loss reduction after each epoch. SFT trainer, or supervised fine-tuning trainer, simplifies and automates many training tasks, making the process more efficient and less error-prone compared to training with PyTorch directly. For the masked language model task your aim should be to predict a masked word using a transformer model. Lets load a masked language model and fine tune it using SFT trainer as shown here. Here you will load the IMDB dataset which will be used to fine-tune the model. Next you are going to define the training arguments object. The training parameters object includes key parameters such as the learning rate and the number of epochs.
Play video starting at :3:24 and follow transcript3:24
Finally, you will define the SFT trainer object. The SFT trainer parameters include the model, training arguments, dataset, and the specific field key you would like to train on. Lets train the model and see the loss for every epoch. You can create a pipeline object mask filler to make predictions. The parameter task specifies the problem type. The input includes the model and the tokenizer which allows you to tokenize the data and make a prediction in one step. To make a prediction, simply input the text, 'This is a [MASK] movie!'. The output result will be an iterable object. The key token STR will contain the predicted token value for the mask and the key score will indicate the likelihood. The samples will be ordered based on the order of likelihood. Observing the output, you can see that great is the most likely token. Let's now recap what you learned. In this video, you learned that Hugging Face is an open source machine learning, or ML, platform with a built-in transformers library for natural language processing, or NLP, applications. Hugging Face's built-in datasets can be loaded using the load_dataset function. The tokenizer function extracts the text from the dataset example and applies the tokenizer. The num_labels parameter specifies the number of classes and determines the number of neurons in the final layer. The evaluation function evaluates the models performance after fine-tuning it. SFT trainer, or supervised fine-tuning trainer, simplifies and automates many training tasks, making the process more efficient and less error prone compared to training with PyTorch directly. The SFT trainer parameters include the model, training arguments, datasets, and the specific field key you would like to the train on. [MUSIC]