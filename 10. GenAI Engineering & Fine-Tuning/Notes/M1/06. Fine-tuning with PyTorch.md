Fine-tuning with PyTorch

Welcome to Fine Tuning with PyTorch. After watching this video, you'll be able to prepare the data set for loading and model definition and describe how to fine tune the complete model and its final layer. Fine tuning in machine learning is the process of adapting a pre train model for specific tasks or use cases. It has become a fundamental deep learning technique, particularly in the training process of foundation models used for generative AI. Let's look at the data sets you will be working with. First is the IMDB data set, which contains details of 50,000 samples of movie reviews. The data set is rather small and includes two classes, positive and negative. The second is the AG news data set, with the details of 120,000 training samples, 7,600 test samples, and four classes, world sports, business, and science and technology. The goal is to pre-train the model on the AG news data set to build a robust understanding of language in context from diverse topics, and then fine tune the IMDB data set to specialize the model for sentiment analysis and move your reviews. Let's first define the class that you're going to use to load the IMDB data set. Now load the IMDB data set into your test and train iterators. Below are a few examples of the data set. Zero indicates a bad review, and one indicates a good review. Next, define the tokenizer and load the glove and beddings. Now, build a vocabulary object from a pre trained glove word embedding model, assigning index values from glove and setting the default index to the token. You can use this code to convert the data set into map style data sets. You can then perform a random split to create separate training and validation data sets and check the device. Let's create the collate function that tokenizes the data set. Converts the tokens to sequences of token indices and transforms these sequences and class labels into tensors. The training validation, and test sets are then converted into data loader objects, which are processed by the collate function. Here is an example from the created validation data loader object. Now you are going to define the transformer based model class. That is the encoder model class for classification and PyTorch. This constructor initializes the test classifier with configurations such as the number of classes, vocabulary size, and transformer settings, like the number of heads and layers. It also sets up the essential components, embeddings, positional encoding, transformer encoder, and a linear classifier for output. The pre-trained word embeddings model, glove is used for the embedding layer. The forward method applies embeddings to the input, adds positional encoding, passes the data through the transformer encoder, averages the data along the first dimension and finally classifies the data using the classifier. The train model function trains a transformer model using the provided optimizer and loss criteria, iterating through the specified number of epoch. It evaluates model performance on a validation data set, prints the loss per epoch and optionally saves the model and performance metrics if the validation accuracy improves. The function predict takes in the text and a text pipeline, which pre processes the text for machine learning. It uses a pre-train model to predict the label of the text for text classification on a data set. Here, you are going to create a function to evaluate the model's accuracy on a data set. Now, let's fine tune the complete model. First, create a model object and PyTorch with four neurons in the output layer. Then using the load state dick method, load the parameters from a pre-trained model on the AG news data set. This method loads the parameters into the model object, allowing you to fine tune it with any data set you choose. As you know, the AG news data set has four classes. Since you're fine tuning the pre-train model on the IMDB data set, which has two classes, you need to change the number of neurons in the final layer from 4-2. It is important to note that you should always adjust the neurons in the final layer according to the number of categories in the data set you're fine tuning on. Now, let's define the loss function, optimizers and scheduler for fine tuning the model with the IMDB data set, then called the train model to finish the model. This step is essentially identical to training a model from scratch. Here, you can see the loss versus accuracy of the model, which achieves approximately 90% accuracy on the validation data. Now let's see what happens if you only fine tune the final layer of the model. For this, you're going to use the same pre-train model on the AG news data set as before. Freeze all layers of the model, as you only need to fine tune the final layer. This is done by selecting the layer parameters and setting the requires grad attribute to false. Freezing all layers except the final one speeds up training by reducing the computation and focusing optimization on fewer parameters. The number of neurons in the output layer has also been changed from 4-2 since the IMDB data set has two categories in the predicted class. Let's define the parameters to retrain the model and fine tune it just for the final layer. In this case, training the model is much quicker. Here is the loss versus accuracy plot of the model. You find tuned only on the final layer. While training is much faster, the performance is significantly worse. Therefore, you must trade off between fine tuning the entire model and certain key parameters. Let's now recap what you've learned. In this video, you learned that. Fine tuning in machine learning is the process of adapting a pre trained model for specific tasks or use cases. The collate function tokenizes the data set, converts the tokens to sequences of token indices and transforms these sequences and class labels into tensors. While defining the transformer based model class for classification and PyTorch, the constructor initializes the text classifier with configurations such as the number of classes, vocabulary size, and transformer settings, like the number of heads and layers. The forward method applies embeddings to the input, adds positional encoding, passes the data through the transformer encoder, averages the data along the first dimension, and finally classifies the data using the classifier. The train model function trains a transformer model, using the provided optimizer and lost criterion, iterating through the specified number of epoch. If you fine tune the complete model, the model achieves approximately 90% accuracy on the validation data. If you only fine tune the final layer of the model, training is much faster but the performance is significantly worse.