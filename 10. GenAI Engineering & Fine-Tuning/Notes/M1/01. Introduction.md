[MUSIC]. Welcome to this course about fine tuning with transformers. Driven by several key trends in AI and natural language processing, or NLP, fine tuning with transformers is becoming a cornerstone of AI strategies across industries. Companies seek to use AI effectively in specialized contexts as large language models, or LLMs, continue evolving and spreading their hands across domains. Here you will learn about AI engineering concepts by working with in demand LLMs and building job ready skills to advance your AI career. This course will focus on encoder models for simplicity, but you can apply these methods to decoder models too. This course is suitable for existing and aspiring data scientists, machine learning engineers, deep learning engineers, AI engineers, and developers who want to excel in working with LLMs. For this course, a basic knowledge of python and PyTorch, an awareness of transformers fine tuning and how to load a model will be an advantage. After completing this course, you will be able to apply the skills you have gained in working with transformer-based LLMs for generative AI engineering. You'll learn to use pretrained transformers for language tasks and then fine tune them for specific tasks. You'll also gain insights into parameter efficient fine tuning or PEFT using low-rank adaptation or LoRA, and quantize low-rank adaptation or QLoRA. You will begin the course with transformers and major language models where you will learn about generative models and fine tuning techniques. You'll review advanced training methods and identify differences between the two powerful frameworks, HuggingFace and PyTorch. You'll also learn how to load a model and its inference and train or pretrain the model using HuggingFace. You'll also understand the importance of fine tuning models using HuggingFace in PyTorch. That's not all, further, you will explore PEFT adapters such as LoRA and QLoRA. You'll gain insights into soft prompts and rank. Additionally, you'll define model quantization using natural language processing or NLP and its unique methods. However, this course won't train on the causal LLMs but these methods are easily transferable. This course consists of labs that reinforce the learning from the instructional videos. The hands-on exercises are based on the Jupyter labs to practice learn concepts and technology. The labs reflect the learning of pretraining and fine tuning LLMs using HuggingFace and PyTorch. The course has the right mix of content to facilitate learning. Videos are short and focused on the main topic. Readings provide detailed content primarily in text format. The labs provide the technical environment, detailed instructions, and code snippets that you can use to complete hands on exercises. The practice and graded quizzes will help you apply what you learned and assess your knowledge. To get the most of your course, watch all videos, complete the labs to practice your new skills, and attempt all quizzes. You can also interact with peers and get help from course staff using the course discussion forums. Let's get started on this exciting journey, good luck. [MUSIC]