# **Loading Models and Inference with Hugging Face Inferences**


Estimated time needed: **20** minutes


In this lab, you will explore how to leverage the Hugging Face `transformers` library for various natural language processing (NLP) tasks. You will start by performing text classification and text generation using pretrained models like DistilBERT and GPT-2 without using the `pipeline()` function, understanding the steps involved in loading models, tokenizing input, performing inference, and processing outputs. Then, you will discover the simplicity and efficiency of using the `pipeline()` function to accomplish the same tasks with minimal code. By comparing both approaches, you will appreciate how the `pipeline()` function streamlines the process, making it easier and faster to implement NLP solutions.


## __Table of Contents__

<ol>
    <li><a href="#Objectives">Objectives</a></li>
    <li>
        <a href="#Setup">Setup</a>
        <ol>
            <li><a href="#Installing-required-libraries">Installing required libraries</a></li>
            <li><a href="#Importing-required-libraries">Importing required libraries</a></li>
        </ol>
    </li>
    <li>
        <a href="#Text-classification-with-DistilBERT">Text classification with DistilBERT</a>
    </li>
    <li>
        <a href="#Text-generation-with-GPT-2">Text generation with GPT-2</a>
    </li>
    <li>
        <a href="#Hugging-Face-pipeline()-function">Hugging Face `pipeline()` function</a>
        <ol>
            <li><a href="#Definition">Definition</a></li>
            <li><a href="#Parameters">Parameters</a></li>
            <li><a href="#Task-types">Task types</a>
                <ol>
                    <li><a href="#Example-1:-Text-classification-using-pipeline()">Example 1: Text classification using `pipeline()`</a>
                    </li>
                    <li><a href="#Example-2:-Language-detection-using-pipeline()">Example 2: Language detection using `pipeline()`</a>
                    </li>
                    <li><a href="#Example-3:-Text-generation-using-pipeline()">Example 3: Text generation using `pipeline()`</a>
                    </li>
                    <li><a href="#Example-4:-Text-generation-using-T5-with-pipeline()">Example 4: Text generation using T5 with `pipeline()`</a>
                    </li>
                </ol>
            </li>
            <li><a href="#Benefits-of-using-pipeline()">Benefits of using `pipeline()`</a></li>
            <li><a href="#When-to-use-pipeline()">When to use `pipeline()`</a></li>
            <li><a href="#When-to-avoid-pipeline()">When to avoid `pipeline()`</a></li>
        </ol>
    </li>
    <li>
        <a href="#Exercise:-Fill-mask-task-using-BERT-with-pipeline()">Exercise: Fill-mask task using BERT with `pipeline()`</a>
    </li>
</ol>


## Objectives

- Learn to set up and use the Hugging Face `transformers` library.
- Perform text classification and text generation using DistilBERT and GPT-2 models without `pipeline()`.
- Understand and utilize the `pipeline()` function to simplify various NLP tasks.
- Compare the ease of using models directly versus using the `pipeline()` function.


----


## Setup


For this lab, you will be using the following libraries:

*   [`torch`](https://pytorch.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for deep learning and neural network modeling.
*   [`transformers`](https://huggingface.co/transformers/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for accessing pretrained models and performing various NLP tasks with ease.


### Installing required libraries



```python
!pip install torch
!pip install transformers
```

    Defaulting to user installation because normal site-packages is not writeable
    Requirement already satisfied: torch in /home/vasim/.local/lib/python3.10/site-packages (2.4.0)
    Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (12.1.105)
    Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (12.1.105)
    Requirement already satisfied: typing-extensions>=4.8.0 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (4.12.2)
    Requirement already satisfied: triton==3.0.0 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (3.0.0)
    Requirement already satisfied: filelock in /home/vasim/.local/lib/python3.10/site-packages (from torch) (3.14.0)
    Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)
    Requirement already satisfied: sympy in /home/vasim/.local/lib/python3.10/site-packages (from torch) (1.12.1)
    Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)
    Requirement already satisfied: jinja2 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (3.1.4)
    Requirement already satisfied: networkx in /home/vasim/.local/lib/python3.10/site-packages (from torch) (3.3)
    Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)
    Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (12.1.105)
    Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (2.20.5)
    Requirement already satisfied: fsspec in /home/vasim/.local/lib/python3.10/site-packages (from torch) (2024.6.0)
    Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vasim/.local/lib/python3.10/site-packages (from torch) (12.1.105)
    Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vasim/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch) (2.0.1)
    Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/vasim/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)
    Defaulting to user installation because normal site-packages is not writeable
    Requirement already satisfied: transformers in /home/vasim/.local/lib/python3.10/site-packages (4.44.2)
    Requirement already satisfied: numpy>=1.17 in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (1.26.4)
    Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (0.24.6)
    Requirement already satisfied: requests in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (2.32.3)
    Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (0.19.1)
    Requirement already satisfied: filelock in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (3.14.0)
    Requirement already satisfied: tqdm>=4.27 in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (4.66.5)
    Requirement already satisfied: packaging>=20.0 in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (24.0)
    Requirement already satisfied: regex!=2019.12.17 in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (2024.7.24)
    Requirement already satisfied: safetensors>=0.4.1 in /home/vasim/.local/lib/python3.10/site-packages (from transformers) (0.4.4)
    Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vasim/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)
    Requirement already satisfied: fsspec>=2023.5.0 in /home/vasim/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vasim/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)
    Requirement already satisfied: charset-normalizer<4,>=2 in /home/vasim/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)


### Importing required libraries

_It is recommended that you import all required libraries in one place (here):_



```python
from transformers import pipeline
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
```

    /home/vasim/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm


# Text classification with DistilBERT


## Load the model and tokenizer

First, let's initialize a tokenizer and a model for sentiment analysis using DistilBERT fine-tuned on the SST-2 dataset. This setup is useful for tasks where you need to quickly classify the sentiment of a piece of text with a pretrained, efficient transformer model.



```python
# Load the tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
```

## Preprocess the input text
Tokenize the input text and convert it to a format suitable for the model:



```python
# Sample text
text = "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim."

# Tokenize the input text
inputs = tokenizer(text, return_tensors="pt")

print(inputs)
```

    {'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,
              2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}


The token ids are the token indexes  ```attention_mask``` is essential for correctly processing padded sequences, ensuring efficient computation, and maintaining model performance. Even when no tokens are explicitly masked, it helps the model differentiate between actual content and padding, which is critical for accurate and efficient processing of input data.


###  Perform inference
The `torch.no_grad()` context manager is used to disable gradient calculation.
This reduces memory consumption and speeds up computation, as gradients are not needed for inference (i.e. when you are not training the model). The **inputs syntax is used to unpack a dictionary of keyword arguments in Python. In the context of the model(**inputs):


Model expects the input like the following: 
```python
# Model expects
with torch.no_grad():
    outputs = model(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"])

# Unpacking with kwargs
with torch.no_grad():
    outputs = model(**inputs)

```

not a dictionary as the tokenizer outputs a dictionary. it is passed as **inputs that unpacks the dictionary into paramters and passed to model. 


```python
# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

outputs
```




    SequenceClassifierOutput(loss=None, logits=tensor([[-3.9954,  4.3336]]), hidden_states=None, attentions=None)



Another method is `input_ids`, and `attention_mask` is their own parameter.



```python
#model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
```

#### Get the logits
The logits are the raw, unnormalized predictions of the model. Let's extract the logits from the model's outputs to perform further processing, such as determining the predicted class or calculating probabilities.



```python
logits = outputs.logits
logits.shape
```




    torch.Size([1, 2])



## Post-process the output
Convert the logits to probabilities and get the predicted class:



### **Understanding `softmax(dim=-1)` Step by Step**

You have this matrix (a **2D tensor**) with **3 rows and 4 columns**:

```
logits = [
  [0.5291, 0.1945, 0.0887, 0.1877],
  [0.1005, 0.6245, 0.1715, 0.1035],
  [0.2503, 0.4760, 0.0901, 0.1836]
]
```

#### **Step 1: What is `dim=-1`?**
- **`dim=-1` means "apply softmax to the last dimension".**  
- In a **2D matrix**, the last dimension (or axis) is the **columns**.
- So, **softmax is applied separately to each row** (not columns).  

---

#### **Step 2: How Does It Work?**
Softmax converts numbers into **probabilities** by making sure each row **sums to 1**.

👉 **Let's apply softmax row by row:**  

For **Row 1** → `[0.5291, 0.1945, 0.0887, 0.1877]`  
- Softmax takes these **four numbers** and transforms them into **four probabilities**.  
- The sum of these **four probabilities** will be **1**.

For **Row 2** → `[0.1005, 0.6245, 0.1715, 0.1035]`  
- Again, softmax applies only to these **four values**, making sure they sum to **1**.

For **Row 3** → `[0.2503, 0.4760, 0.0901, 0.1836]`  
- The same process happens: softmax turns these four numbers into **four probabilities** that sum to **1**.

---

#### **Step 3: Why Do We Apply Softmax Like This?**
- In **machine learning (ML)**, especially in classification tasks, each row represents a **different sample**.
- Each **column** represents a different **class**.
- Softmax ensures that for each row (each sample), the numbers **turn into probabilities** that add up to **1**.

✅ **So, using `softmax(dim=-1)` means:**
- "Apply softmax across columns, treating each row separately."
- "Make each row sum to **1**, so we get valid probability values."

---

### **Visual Example**
Imagine each row is a different student taking a test, and each column represents a different subject:

| Student | Math | Science | History | English |
|---------|------|---------|---------|---------|
| **A**  | 0.5  | 0.2     | 0.1     | 0.2     |
| **B**  | 0.1  | 0.6     | 0.2     | 0.1     |
| **C**  | 0.2  | 0.5     | 0.1     | 0.2     |

Softmax ensures that **each student's row sums to 1**, making it a **valid probability distribution**.

---

### **What Happens If We Use `dim=0` Instead?**
- `dim=0` means **apply softmax across rows (column-wise).**
- This would mean comparing **Math scores across all students**, then **Science scores across all students**, etc.
- This is usually **not** what we want in classification tasks.

---

### **Key Takeaways for Beginners**
✅ **`dim=-1` applies softmax row-wise (across columns)**.  
✅ **Each row is treated separately, and softmax converts values into probabilities that sum to 1**.  
✅ **This is useful in ML because each row represents a different sample's class probabilities**.  



```python
# Convert logits to probabilities
probs = torch.softmax(logits, dim=-1)

# Get the predicted class
predicted_class = torch.argmax(probs, dim=-1)

# Map the predicted class to the label
labels = ["NEGATIVE", "POSITIVE"]
predicted_label = labels[predicted_class]

print(f"Predicted label: {predicted_label}")
```

    Predicted label: POSITIVE


# Text generation with GPT-2 


## Load tokenizer
 Load the pretrained GPT-2 tokenizer. The tokenizer is responsible for converting text into tokens that the model can understand.



```python
# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
```

Load the pretrained GPT-2 model with a language modeling head. The model generates text based on the input tokens.



```python
# Load the tokenizer and model

model = GPT2LMHeadModel.from_pretrained("gpt2")
```

## Preprocess the input text  
Tokenize the input text and convert it to a format suitable for the model, like before you have the token indexes, i.e., inputs. 



```python
# Prompt
prompt = "Once upon a time"

# Tokenize the input text
inputs = tokenizer(prompt, return_tensors="pt")
inputs
```




    {'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}



## Perform inference  
Generate text using the model

```inputs:``` Input token IDs from the tokenizer

```attention_mask:``` Mask indicating which tokens to attend to

```pad_token_id:```Padding token ID set to the end-of-sequence token ID

```max_length:``` Maximum length of the generated sequences

```num_return_sequence:``` Number of sequences to generate



```python
tokenizer.eos_token_id
```




    50256




```python
tokenizer.eos_token
```




    '<|endoftext|>'




```python
# Generate text
output_ids = model.generate(
    inputs.input_ids, 
    attention_mask=inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    max_length=50, 
    num_return_sequences=1
)

output_ids
```




    tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,
             8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,
             3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049, 3514,   13,
              383,  995,  373,  257, 1295,  286, 1049, 3514,   11,  290,  262,  995,
              373,  257]])



The `pad_token_id` parameter in the `model.generate` function is used to specify the token ID that should be used for padding. Padding tokens are typically used to ensure that all input sequences in a batch have the same length, which is necessary for efficient processing in many machine learning models.

In this context, setting `pad_token_id=tokenizer.eos_token_id` means that the end-of-sequence (EOS) token ID defined by the tokenizer will be used as the padding token. This ensures that any padding added to the input sequences will be recognized as such by the model, allowing it to correctly handle the padding during text generation.

The significance of using the EOS token ID as the padding token is that it helps the model distinguish between actual content and padding. This is particularly important when generating text, as it ensures that the model does not generate text based on padding tokens, which could lead to incorrect or nonsensical output.

or


```python
with torch.no_grad():
    outputs = model(**inputs) 

outputs


## Post-process the output  
Decode the generated tokens to get the text:



```python
# Decode the generated text
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(generated_text)
```

    Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a


# Hugging Face `pipeline()` function

The `pipeline()` function from the Hugging Face `transformers` library is a high-level API designed to simplify the usage of pretrained models for various natural language processing (NLP) tasks. It abstracts the complexities of model loading, tokenization, inference, and post-processing, allowing users to perform complex NLP tasks with just a few lines of code.

## Definition

```python
transformers.pipeline(
    task: str,
    model: Optional = None,
    config: Optional = None,
    tokenizer: Optional = None,
    feature_extractor: Optional = None,
    framework: Optional = None,
    revision: str = 'main',
    use_fast: bool = True,
    model_kwargs: Dict[str, Any] = None,
    **kwargs
)
```

## Parameters

- **task**: `str`
  - The task to perform, such as "text-classification", "text-generation", "question-answering", etc.
  - Example: `"text-classification"`

- **model**: `Optional`
  - The model to use. This can be a string (model identifier from Hugging Face model hub), a path to a directory containing model files, or a pre-loaded model instance.
  - Example: `"distilbert-base-uncased-finetuned-sst-2-english"`

- **config**: `Optional`
  - The configuration to use. This can be a string, a path to a directory, or a pre-loaded config object.
  - Example: `{"output_attentions": True}`

- **tokenizer**: `Optional`
  - The tokenizer to use. This can be a string, a path to a directory, or a pre-loaded tokenizer instance.
  - Example: `"bert-base-uncased"`

- **feature_extractor**: `Optional`
  - The feature extractor to use for tasks that require it (e.g., image processing).
  - Example: `"facebook/detectron2"`

- **framework**: `Optional`
  - The framework to use, either `"pt"` for PyTorch or `"tf"` for TensorFlow. If not specified, it will be inferred.
  - Example: `"pt"`

- **revision**: `str`, default `'main'`
  - The specific model version to use (branch, tag, or commit hash).
  - Example: `"v1.0"`

- **use_fast**: `bool`, default `True`
  - Whether to use the fast version of the tokenizer if available.
  - Example: `True`

- **model_kwargs**: `Dict[str, Any]`, default `None`
  - Additional keyword arguments passed to the model during initialization.
  - Example: `{"output_hidden_states": True}`

- **kwargs**: `Any`
  - Additional keyword arguments passed to the pipeline components.

## Task types

The `pipeline()` function supports a wide range of NLP tasks. Here are some of the common tasks:

1. **Text Classification**: `text-classification`
   - **Purpose**: Classify text into predefined categories.
   - **Use Cases**: Sentiment analysis, spam detection, topic classification.

2. **Text Generation**: `text-generation`
   - **Purpose**: Generate coherent text based on a given prompt.
   - **Use Cases**: Creative writing, dialogue generation, story completion.

3. **Question Answering**: `question-answering`
   - **Purpose**: Answer questions based on a given context.
   - **Use Cases**: Building Q&A systems, information retrieval from documents.

4. **Named Entity Recognition (NER)**: `ner` (or `token-classification`)
   - **Purpose**: Identify and classify named entities (like people, organizations, locations) in text.
   - **Use Cases**: Extracting structured information from unstructured text.

5. **Summarization**: `summarization`
   - **Purpose**: Summarize long pieces of text into shorter, coherent summaries.
   - **Use Cases**: Document summarization, news summarization.

6. **Translation**: `translation_xx_to_yy` (e.g., `translation_en_to_fr`)
   - **Purpose**: Translate text from one language to another.
   - **Use Cases**: Language translation, multilingual applications.

7. **Fill-Mask**: `fill-mask`
   - **Purpose**: Predict masked words in a sentence (useful for masked language modeling).
   - **Use Cases**: Language modeling tasks, understanding model predictions.

8. **Zero-Shot Classification**: `zero-shot-classification`
   - **Purpose**: Classify text into categories without needing training data for those categories.
   - **Use Cases**: Flexible and adaptable classification tasks.

9. **Feature Extraction**: `feature-extraction`
   - **Purpose**: Extract hidden state features from text.
   - **Use Cases**: Downstream tasks requiring text representations, such as clustering, similarity, or further custom model training.


### Example 1: Text classification using `pipeline()`

In this example, you will use the `pipeline()` function to perform text classification. You will load a pretrained text classification model and use it to classify a sample text.

#### Load the text classification model:
We initialize the pipeline for the `text-classification` task, specifying the model `"distilbert-base-uncased-finetuned-sst-2-english"`. This model is fine-tuned for sentiment analysis.

#### Classify the sample text:
We use the classifier to classify a sample text: "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim." The `classifier` function returns the classification result, which is then printed.



```python
# Load a general text classification model
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

# Classify a sample text
result = classifier("Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.")
print(result)
```

    [{'label': 'POSITIVE', 'score': 0.9997586607933044}]


#### Output

The output will be a list of dictionaries, where each dictionary contains:

- `label`: The predicted label (e.g., "POSITIVE" or "NEGATIVE").
- `score`: The confidence score for the prediction.


### Example 2: Language detection using `pipeline()`

In this example, you will use the `pipeline()` function to perform language detection. You will load a pretrained language detection model and use it to identify the language of a sample text.

#### Load the language detection model:
We initialize the pipeline for the `text-classification` task, specifying the model `"papluca/xlm-roberta-base-language-detection"`. This model is fine-tuned for language detection.

#### Classify the sample text:
We use the classifier to detect the language of a sample text: "Bonjour, comment ça va?" The `classifier` function returns the classification result, which is then printed.



```python
from transformers import pipeline

classifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
result = classifier("Bonjour, comment ça va?")
print(result)
```

    [{'label': 'fr', 'score': 0.9934879541397095}]


#### Output
The output will be a list of dictionaries, where each dictionary contains:

- `label`: The predicted language label (e.g., "fr" for French).
- `score`: The confidence score for the prediction.


### Example 3: Text generation using `pipeline()`

In this example, you will use the `pipeline()` function to perform text generation. You will load a pretrained text generation model and use it to generate text based on a given prompt.

#### Initialize the text generation model:
We initialize the pipeline for the `text-generation` task, specifying the model `"gpt2"`. GPT-2 is a well-known model for text generation tasks.



```python
# Initialize the text generation pipeline with GPT-2
generator = pipeline("text-generation", model="gpt2")
```

#### Generate text based on a given prompt:
We use the generator to generate text based on a prompt: "Once upon a time". Let's specify `max_length=50`, `truncation=True` to limit the generated text to 50 tokens and `num_return_sequences=1` to generate one sequence. The `generator` function returns the generated text, which is then printed.



```python
# Generate text based on a given prompt
prompt = "Once upon a time"
result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)

# Print the generated text
print(result[0]['generated_text'])
```

    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.


    Once upon a time, some of us had been told by my own son, that a beautiful bride would come and show us her beauty. There were so many such illusions that many women found it easy to go to a fair and pleasant place with men


#### Output
The output will be a list of dictionaries, where each dictionary contains:

- `generated_text`: The generated text based on the input prompt.


### Example 4: Text generation using T5 with `pipeline()`

In this example, you will use the `pipeline()` function to perform text-to-text generation with the T5 model. You will load a pretrained T5 model and use it to translate a sentence from English to French based on a given prompt.

#### Initialize the text generation model:
We initialize the pipeline for the `text2text-generation task, specifying the model "t5-small". T5 is a versatile model that can perform various text-to-text generation tasks, including translation.



```python
# Initialize the text generation pipeline with T5
generator = pipeline("text2text-generation", model="t5-small")
```

#### Generate text based on a given prompt:
We use the generator to translate a sentence from English to French based on the prompt: "translate English to French: How are you?". Let's specify `max_length=50` to limit the generated text to 50 tokens and `num_return_sequences=1` to generate one sequence. The `generator` function returns the translated text, which is then printed.



```python
# Generate text based on a given prompt
prompt = "translate English to French: How are you?"
result = generator(prompt, max_length=50, num_return_sequences=1)

# Print the generated text
print(result[0]['generated_text'])
```

#### Output
The output will be a list of dictionaries, where each dictionary contains:

- `generated_text`: The generated text based on the input prompt.


## Benefits of using `pipeline()`

- **Reduced Boilerplate Code**: Simplifies the code required to perform NLP tasks.
- **Improved Readability**: Makes code more readable and expressive.
- **Time Efficiency**: Saves time by handling model loading, tokenization, inference, and post-processing automatically.
- **Consistent API**: Provides a consistent API across different tasks, allowing for easy experimentation and quick prototyping.
- **Automatic Framework Handling**: Automatically handles the underlying framework (TensorFlow or PyTorch).

## When to use `pipeline()`

- **Quick Prototyping**: When you need to quickly prototype an NLP application or experiment with different models.
- **Simple Tasks**: When performing simple or common NLP tasks that are well-supported by the `pipeline()` function.
- **Deployment**: When deploying NLP models in environments where simplicity and ease of use are crucial.

## When to avoid `pipeline()`

- **Custom Tasks**: When you need to perform highly customized tasks that are not well-supported by the `pipeline()` function.
- **Performance Optimization**: When you need fine-grained control over the model and tokenization process for performance optimization or specific use cases.


# Exercise: Fill-mask task using BERT with `pipeline()`

In this exercise, you will use the `pipeline()` function to perform a fill-mask task using the BERT model. You will load a pretrained BERT model and use it to predict the masked word in a given sentence.


### Instructions

1. **Initialize the fill-mask pipeline** with the BERT model.
2. **Create a prompt** with a masked token.
3. **Generate text** by filling in the masked token.
4. **Print the generated text** with the predictions.



```python
# TODO
```

<details>
    <summary>Click here for Solution</summary>

```python
# Initialize the fill-mask pipeline with BERT
fill_mask = pipeline("fill-mask", model="bert-base-uncased")

# Generate text by filling in the masked token
prompt = "The capital of France is [MASK]."
result = fill_mask(prompt)

# Print the generated text
print(result)
```

</details>


## Authors


[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.


© Copyright IBM Corporation. All rights reserved.

