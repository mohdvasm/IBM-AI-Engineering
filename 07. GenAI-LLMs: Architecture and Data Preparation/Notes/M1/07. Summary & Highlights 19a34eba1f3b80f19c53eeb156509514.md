# C7M1.7. Summary & Highlights

Last edited: February 15, 2025 1:37 AM
Tags: Course 07

# Summary and Highlights

Congratulations! You have completed this lesson. At this point in the course, you know that:

- Generative AI refers to deep-learning models that can generate content, such as text, images, audio, 3D objects, and music, based on the training data.
- These models understand the relationship between words and phrases and generate contextually relevant text. An example of a generative AI model for text generation is a generative pre-trained transformer (GPT).
- These models can generate images from text input and seed images. Examples of generative AI models for image generation are data analysis learning with language model for generation and exploration (DALL-E) and generative adversarial networks (GANs).
- Generative AI models can generate natural-sounding speech. An example of this type of generative AI model is WaveNet.
- Generative AI architectures and models include recurrent neural networks (RNNs), transformers, GANs, variational autoencoders (VAEs), and diffusion models.
    - RNNs use sequential or time series data and a loop-based design for training.
    - Transformers utilize the self-attention mechanism to focus on the most important parts of the information.
    - GANs consist of a generator and a discriminator, which work in a competitive mode.
    - VAEs operate on an encoder-decoder framework and create samples based on similar characteristics.
    - Diffusion models generate creative images by learning to remove noise and reconstruct distorted examples, relying on statistical properties.
- Generative AI started with rule-based systems that use predefined linguistic rules, followed by machine-learning approaches focusing on statistical methods. It later moved to deep learning, which uses neural networks trained on extensive data sets. Transformers represent the latest in this evolution.
- The evolution of generative AI has led to advancements in machine translation, chatbot conversations, sentiment analysis, and text summarization.
- Large language models (LLMs) are foundation models that use AI and deep learning with vast data sets. LLMs have training data sets that run into petabytes and contain billions of parameters. Examples of LLMs are GPT, Bidirectional Encoder Representations from Transformers (BERT), Bidirectional and Auto-Regressive Transformers (BART), and Text-to-Text Transfer Transformer (T5).
- Some libraries and tools you can use to implement generative AI for NLP are PyTorch, TensorFlow, Hugging Face, LangChain, and Pydantic.