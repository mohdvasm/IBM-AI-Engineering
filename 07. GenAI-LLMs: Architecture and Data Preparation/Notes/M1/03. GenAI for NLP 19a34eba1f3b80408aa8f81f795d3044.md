# C7M1.3. GenAI for NLP

Last edited: February 15, 2025 1:37 AM
Tags: Course 07

Welcome to Generative AI for NLP. After watching this video, you'll be able to describe the role and evolution of generative AI architecture in the context of natural language processing, or NLP. You'll be able to explain the applications of these architectures in performing language related tasks, and describe the significance of large language models or LLMs. Imagine you work in a bank as an AI engineer, the bank has assigned you the task of creating a virtual assistant that can engage in natural language conversations with customers, answering their queries on account details, investment options and so on. You'll explore how you can use generative AI architectures to develop applications that interact with humans in natural language. Generative AI architectures enable machines to comprehend human language and generate responses, you cannot distinguish from what a human generates. They improve language processing by incorporating context awareness, and ensuring coherent interactions. They enable meaningful conversations through predictive analytics and advanced modeling. The NLP systems based on generative AI architectures sense the feelings and grasp the intentions behind words, extending their understanding beyond mere words. Let's look at the evolution of generative AI for NLP. It started with rule-based systems that strictly follow predefined linguistic rules, such as grammar. While these systems were precise, they lacked flexibility. Next, machine learning based approaches emerged, these approaches employed statistical methods to learn from vast language datasets, and make predictions. These were more adaptable than rule-based systems, but still limited in understanding complex language nuances. A significant advancement happened with the introduction of deep learning. Deep learning focuses on training artificial neural networks with extensive data sets. It involves many computational units within these networks, working together to make more nuanced language interpretations. Transformers represent the latest in this evolution, the transformer architecture is specifically designed to handle sequential data. Transformers have a greater ability to understand context and dependencies within language than their predecessors. Generative AI for NLP is constantly evolving due to consistent efforts, to improve how machines understand and generate language. You'll notice this evolution in the significant advancements in machine translation, chatbot conversation, sentiment analysis, and text summarization. In machine translation, these architectures significantly enhance accuracy by enabling more precise and context aware conversions between languages. In chatbots or virtual assistants, you can utilize generative AI architectures to make the conversations more natural and humanlike with a degree of empathy and personalization, enhancing the user experience. The ability of generative AI architectures to grasp subtle language expressions improves the effectiveness of sentiment analysis, offering deeper insights into user sentiments. In text summarization, you can employ these architectures to enable recognition of the core meaning and significance of text in a document, resulting in a more precise summary. Generative AI architectures include language models focusing on understanding and generating human language. Large language models, or LLMs, are foundation models that use AI and deep learning with vast datasets such, as websites and books to generate text, translate languages, and create various types of content. They are referred to as large language models due to the size of the training data set, which may reach petabytes. Also these models contain billions of parameters, which are variables defining the model's behavior. These parameters are fine tuned during training to optimize the model's performance on specific tasks. For example, when the model learns about emotions, a parameter could represent the weights assigned to specific words, such as happy or sad. The extensive training of LLMs on massive datasets enables them to understand language structures and contexts comprehensively. They can capture the nuances of human language, facilitating more natural interactions. LLMs are proficient in predicting the next word in a sequence, with their vast resources, LLMs can produce creative content with minimal task specific training. Some examples of LLMs are the generative pretrained transformer series or GPT series. By directional encoder representation from transformers or BERT, By directional and autoregressive transformers or BERT and text to text transfer transformer, also called T5. GPT primarily acts as a decoder adept at generating text. It excels in tasks where creating coherent and contextually relevant content is crucial for example, chatbots. BERT utilizes an encoder only transformer architecture. It is exceptional at understanding the context of a word within a sentence, which is crucial for nuanced tasks like sentiment analysis, and question answering. Models such as BERT and T5 follow an encoder decoder architecture. They leverage encoding for contextual understanding and decoding to generate texts. This versatility makes them well suited for various NLP tasks. The term GPT and ChatGPT might sound interchangeable, though these models share similarities, GPT focuses on diverse text generation tasks, whereas ChatGPT focuses on generating conversations. For training and fine tuning GPT predominantly uses supervised learning, it might use reinforcement learning, but with less focus on conversational aspects. ChatGPT uses a combination of supervised learning and reinforcement learning. GPT does not incorporate feedback from human interactions as part of its learning process, whereas Chat GPT uses a methodology called reinforcement learning from human feedback, or RLHF, which uses human feedback to create a reward model. Most of the LLMs are based on the transformer architecture. The versatility of LLMs makes them a key contributor to advancements in natural language understanding and generation. You can pretrain LLMs for generic purposes, and then fine tune with a much smaller data set. For example, the LLMs may be trained in generic text classification. You can finetune them in a retail industry context for categorizing products into groups, such as electronics or clothing based on textual descriptions. Remember that while these models can generate authoritative text across domains, they may also generate information that sounds right but isn't accurate. You may also need to address biases and consider the potential impact of the generated content on society. Let's now recap what you learned, in this video you learned about generative AI architectures for NLP. Generative AI architectures enable machines to comprehend human language and generate responses you cannot distinguish from what a human generates. Generative AI started with rule based systems that follow predefined linguistic rules. Then machine learning approaches emerged, focusing on statistical methods. A significant leap happened with deep learning, which uses neural networks trained on extensive datasets. Transformers represent the latest in this evolution. The enhanced capabilities of generative AI architectures have led to significant advancements in machine translation, chatbot conversation, sentiment analysis, and text summarization. LLMs are foundation models that use AI in deep learning with vast data sets. They're called large language models due to the size of the training data set, which may run into petabytes. Also, they have billions of parameters examples of LLMs are GPT, BERT, BART, and T5.