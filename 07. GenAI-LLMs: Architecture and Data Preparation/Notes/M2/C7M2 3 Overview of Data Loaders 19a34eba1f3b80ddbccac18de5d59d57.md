# C7M2.3. Overview of Data Loaders

Last edited: February 15, 2025 1:38 AM
Tags: Course 07

Welcome to overview of data loaders. After watching this video, you'll be able to define a data loader, explain its purpose, and describe the data loader class and batch functions. Imagine you're developing a machine translation application with huge datasets. It's challenging to manually load and shuffle these datasets to train the underlying language model. How can you manage this process efficiently? A data loader helps you prepare and load data, leading frameworks such as PyTorch have a dedicated data loader class that you can use to handle and prepare data while training generative AI models. Natural language processing or NLP data loaders and PyTorch enable efficient loading and pre-processing of textual data for NLP tasks. Data loaders enable efficient batching and shuffling of data which is essential for training neural networks. They allow for on-the-fly pre-processing, which optimizes memory usage by loading only the required data during training. Data loaders seamlessly integrate with the PyTorch training pipeline, making it easier to train and evaluate models. They simplify data augmentation and pre-processing, allowing you to apply various transformations to the input data. In PyTorch, you'll work with datasets and data loaders for efficient data handling, a dataset which is a collection of data samples and their labels is the starting point. Let's consider three text samples. The task is to classify whether they are grammatically correct or incorrect. You typically divide your dataset into a training set used to train the model, a validation set to tweak and validate the model parameters, and a test set to assess the model's performance in real-world scenarios. The terms training set and validation set can sometimes be used interchangeably, although they have distinct functions in model training and evaluation. This example shows how to create a custom dataset and use the data loader class in PyTorch. The dataset consists of a list of random sentences. The objective is to generate batches of sentences for further processing, such as training a neural network model. The code defines a custom dataset called CustomDataset. This dataset inherits from the torch.utils.data.dataset class and is initialized with a list of sentences. The dataset comprises three essential methods. The init method initializes the dataset with a list of sentences. The len method returns the number of samples. The getitem method retrieves an item, in this case, a sentence at a specific index idx. You can create a dataset object and access the first sample, like in a list, you can access the second sample, and so on. Using data loaders, you can output data in batches instead of one sample at a time. A data loader is an iterator object and PyTorch used for loading shuffling and batching data from a dataset, facilitating training on groups of samples. An iterator is an object that can be looped over, it contains elements that can be iterated through and typically includes two methods, iter and next. You commonly use iterators to traverse large datasets. Each time you call the next function, it returns new batches of samples. Calling next again will fetch the next item in the iterator, this process continues. Next, you create an instance of your custom data set by passing the list of sentences. Additionally, you specify a batch size which determines how many sentences will be grouped in each batch during data loading. You can then create a data loader by providing your custom dataset and batch size to the torch.utails.data.dataloader class. In addition, you can set shuffle is true. It's a randomly shuffle the sentences before they are divided into batches. The shuffling is particularly useful for training deep learning models, as it prevents the model from learning patterns based on the order of the data. Then iterate through the data loader to see how data is loaded in batches. During iteration, the sentences in each batch are printed to illustrate how the data loader groups and presents the data. In most NLP applications, the majority of data transformation is performed in the batch function. You use data loaders for various transformations on input text data. These transformations include tokenizing the text, numericalizing it, resizing it to a consistent size, and converting it into tensors. These pre-processing steps ensure that data is prepared in a format suitable for deep learning models to process and interpret. The code defines the tokenizer using the get tokenizer function with the basic English option. Next, it builds a vocabulary from the sentences using the build vocab from iterator function. This function constructs the vocabulary from the tokenized sentences. After tokenizing the input data, the sequences may not be the same size. Each sample in a data loader must be the same length. Hence, you would use padding. You can utilize the pad sequence function in PyTorch, which pad sequences in a batch to match the length of the longest sequence. The padding value equals zero argument specifies the value to use for padding. The batch first argument guarantees that the batch dimension is the first dimension in the output tensor. When you set batch first to true, the first dimension in the output tensor will represent the batch size. Conversely, if the default value for batch first is false, the first dimension in the output tensor will represent the sequence size and the batch size will become the second dimension. Let's see an example of how batch first works with the batch first argument as true. The first dimension in the output tensor is the batch size, which is two here. When you set batch first to false, its default value, the first dimension in the output tensor represents the length of the input sequences in the batch, in this case, 10. To keep the original dataset untouched, you can take care of data transformation in the collate function. You also have the option to utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor. Let's look at an example of a custom collate function that performs these tasks. The code defines a custom collate function named collate_fn, within the function, each sample in the batch is tokenized using the tokenizer function. Next, tokens are mapped to numbers using the vocab. Then the pad sequences function is applied to the tensor batch to pad the sequences within the batch to have equal lengths. Now you can create a data loader using the collate function and the custom dataset. Let's now recap what you learned. In this video, you learned about data loaders. A data loader helps you prepare and load data to train generative AI models. PyTorch and TensorFlow have a dedicated data loader class. Data loaders enable efficient batching and shuffling of data and allow for on-the-fly processing. Data loaders seamlessly integrate with the PyTorch training pipeline and simplify data augmentation and pre-processing. Using data loaders, you can output data in batches instead of one sample at a time.