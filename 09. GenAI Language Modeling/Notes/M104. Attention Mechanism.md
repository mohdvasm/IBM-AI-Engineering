Welcome to attention mechanism. After watching this video, you'll be able; to explain how attention mechanism works in language translation, describe how to apply attention mechanism to word embeddings, describe how to apply attention mechanism to sequences. Imagine conversing with your colleagues in a crowded hall. How do you communicate despite the noise around you? You pay more attention to your colleagues conversations than background discussions. Similarly, generative AI models use attention mechanisms to focus on the most relevant parts of the input data and their relationships. Let's start exploring attention mechanisms by drawing parallels with how Python dictionaries help in translation. Python dictionaries help with tasks such as translation by pairing keys with their corresponding values. When you query the dictionary, you input the key. In return, the dictionary provides the value that corresponds to that key. In this instance, the keys represent French words, and the values are the English translations. To translate a French word to English, query the dictionary with the French term. The dictionary returns the English translation. You can think of the dictionary as having two columns. The left column contains the keys, which are words in French, whereas the right column contains the corresponding English translations. Each row represents a distinct word pairing. Notice that every key is matched with its value. At the bottom, you can see the actual query input into the dictionary. Chat is rendered as cat, sous as under, and both la and le point to the. The attention mechanism uses a comparable structure, but instead of strings, you'll employ one-hot encoded vectors. Each French word in the dictionary is represented as a one-hot encoded vector called keys. These individual keys are assembled into a matrix K, each row corresponding to the one-hot encoded word. Like a Python dictionary, the query vectors are identical to the key vectors, represented with Q instead of K. The English words are also represented by one-hot encoded vectors called values. These individual values are then assembled into a matrix called V, each row containing the one-hot encoded word. The key matrix K and the value matrix V must be arranged in such a way that the query vector for the word intended for translation aligns with the same row across both matrices. You can see that the row for chat aligns with cat, est with is, and sous with under. The English article the is represented twice within the value matrix V. Next, you'll transpose the key matrix K converting the rows to columns. This is a shortcut for performing a series of dot products. Let's examine the attention mechanism formula for translation. It's the product of the query vector for the word, the transpose of K and the V matrix. The result will be a vector. Let's refer to it as h, representing the value vector corresponding to the translated word. First, you insert the one-hot encoded French query word you intend to translate to. Then you multiply the query vector by the transposed key matrix. The key matrix is transposed, so each column corresponds to the queries for the labeled words. Similarly, the value rows are labeled with the corresponding word that they represent. The attention formula involves computing the dot product across the key vectors that form the columns of matrix K. Due to their orthogonality, all values in the resultant row vector are zero, except for the position where the query vector perfectly matches a key vector. Multiplying this non-zero element by the value matrix isolates the corresponding translated word vector. You can now have the word vector for the translated word in this case, under. To retrieve the translated word from the translated vector, you can apply the equation Omega hat equals argmax_i(hV transpose), where h is the output of the attention mechanism, and V is the value matrix. When multiplying h with V transpose, you perform a dot product with every value, as each element is orthogonal except for the word with the same value. You get a vector of zero except for the one that is the same. This column has the same index as the word index of the translated word in the value matrix under in this case. With a few adjustments, you can apply the attention mechanism to word embeddings. This process helps capture contextual relationships between words and thus enables the translation of words the model has not encountered before. You substitute the keys and values with word embeddings, aligning the rows with their respective translation. You will then apply the attention formula to the word embedding for ci-dessous, a word absent from the key matrix. This word translates to below. Let's see the product of the query matrix with the key matrix. You perform a dot product with each term in the key matrix. When you examine the output, you'll see that the maximum value corresponds to the column for sous, which means below. This correlation makes sense as the words share similar embedding values. Remember, you require a one-hot encoded vector to select the precise word from the value matrix. You can refine the attention formula by incorporating the softmax function on the resultant output of the dot product between the query vector and the key. Let's see what happens in the softmax operation. You can refer to this resultant vector as Alpha. In simple terms, you perform the matrix product on the query and the key and then apply the softmax function on the resultant vector Z. The softmax function accentuates the largest value by scaling it close to one while diminishing the smaller values closer to zero. The function helps amplify the largest value relative to the rest. When you apply the softmax function to the resultant vector Z, it transforms the column with the largest Alpha into a form that resembles a binary vector. You can now use the attention mechanism to translate the word ci-dessous. Consider the output labeled h. Given that this embedding closely resembles that of sous, you identify the column with the highest value. Implementing the softmax function simplifies this into a one-hot vector. Multiply this one-hot vector with the value matrix to retrieve the word embedding for the translated word under. This describes a search method that identifies the most similar vector using an approach akin to one-hot encoding. Because the resulting vector h corresponds to the intended word, performing the dot product with h will yield the highest value at the index of the target word. Let's now look at attention for sequences. You can consolidate all the query vectors into a single matrix, denoted as Q to process every vector concurrently within a single matrix product operation. The input values are treated as a sequence of embeddings depicted in pink. The process yields a set of refined embeddings tailored for specific tasks, illustrated in blue. It's important to note that the actual translation method in transformers slightly differs. In addition, positional encodings can be added. However, for simple data problems, these positional encodings may not be necessary. Let's now recap what you learned. In this video, you learned that attention mechanisms employ the query, key and value matrices. The query vector should align with the same row across key and value matrices. You can apply attention mechanism to word embeddings. This process helps capture contextual relationships between words. You can refine the attention formula by incorporating the softmax function on the output of the dot product between the query vector and the key. For employing attention for sequences, you can consolidate all the query vectors into a single matrix.