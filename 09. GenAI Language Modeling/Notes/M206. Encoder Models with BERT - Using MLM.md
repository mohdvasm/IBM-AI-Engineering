Welcome to encoder models with Bert. Pre training using MLM after watching this video, you will be able to explain Bert models and their encoder only architecture. You will also learn to train BERT models using masked language modeling or MLM. Bidirectional encoder representations from transformers or BERT developed by Google, revolutionized NLP with its deep understanding of word context and semantics. It's pre-trained via self supervised learning on vast text corpora using masked language modeling MLM and next sentence prediction. BERT's architecture allows for fine-tuning specific tasks like text summarization, question answering, and sentiment analysis, adapting its comprehensive knowledge to specialized applications. BERT utilizes an encoder only architecture that is only the encoder part of the transformer model. This design allows BERT to process entire sequences of text simultaneously, theoretically enhancing its understanding of the context and nuances within the text. Unlike autoregressive models, BERT is not typically used for text generation tasks. Instead, it excels at a broad range of language comprehension tasks. Consider the task of predicting a known word denoted by the token MASK within the input CLS, IBM, MASK, me, BERT, SEP. An autoregressive model would only have access to the CL's and IBM tokens to predict the masked word. However, Bert can utilize the full context from both sides of the masked token, which allows for a more informed prediction, in this case, taut. One other difference is that the BERT models contain the segment embeddings used in training. Lets learn about MLM. It involves randomly masking some of the input tokens and training BERT to predict the original mask tokens. This task helps Bert learn contextual representations and understand the relationships between words. The prediction method is almost identical to that used in decoder models like GPT. The encoder outputs a set of contextual embeddings depicted in gray, which are then passed through another layer and converted into a set of logits shown in red. The masked word is identified by selecting the word corresponding to the index with the highest logit value. The main distinction lies in the fact that encoder models have access to the entire sequence. A hallmark of encoder models like BERT is their bi-directional training method, which enables the model to understand the context from both sides of any given word in a sentence. For example, take the task of predicting the missing word in the farmers cultivate the question mark to grow crops. In contrast to models such as GPT, which only consider the preceding text, farmers cultivate the due to their causal attention mechanism, BERTS architecture allows it to analyze text from both directions. And decoder models like GPT causal attention is visually represented by an X for the masked attention and OS for the active attention units. BERT, however, employs a bi-directional mechanism without such masking, fully utilizing the context around each word as depicted by the presence of only Os. This bi-directional approach gives Bert a more sophisticated grasp of language nuances and complex word interdependencies. According to the original BERT paper, you should randomly hide 15% of the input words and try to predict them. You can use the hidden vectors of these masked words to make predictions using a vocabulary. Instead of reconstructing the entire input, you should only focus on predicting the masked words. Although this allows you to obtain a bi-directional pre-trained model. A downside is that you are creating a mismatch between pre-training and fine-tuning since the mask token does not appear during fine-tuning. To address this, don't always replace masked words with the mask token. When generating training data, 15% of the word positions are randomly chosen. If a word is chosen, you can replace it with the mask token 80% of the time, a random token 10% of the time and leave it unchanged 10% of the time. This data is used to predict the original word using cross entropy loss. Let's consider an example to illustrate. 85% of the words remain unchanged and from the remaining 15% 80% are replaced with mask 10% with a random token like the and 10% are used for prediction without any changes like cultivate. Let's now recap what you learned. In this video, you learned that bi-directional encoder representations from transformers or BERT's architecture allow for fine-tuning specific tasks like text summarization, question answering, and sentiment analysis. BERT utilizes an encoder only architecture which allows it to process entire sequences of text simultaneously, theoretically enhancing its understanding of the context and nuances within the text. Masked language modeling, or MLM, involves randomly masking some of the input tokens and training BERT to predict the original masked tokens. The prediction method is as the encoder outputs a set of contextual embeddings, which are then passed through another layer and converted into a set of logits. The masked word is identified by selecting the word corresponding to the index with the highest logit value. The main distinction between decoder and encoder models lies in the fact that encoder models have access to the entire sequence. A hallmark of encoder models like BERT is their bi-directional training method, which enables the model to understand the context from both sides of any given word in a sentence. [MUSIC]