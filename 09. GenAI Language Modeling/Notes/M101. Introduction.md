 # Introduction
 
 Welcome to this course on language modeling with transformers. Here, you will learn about the fundamental and advanced concepts of transformer-based models for NLP. This course is suitable for existing and aspiring data scientists, machine learning engineers, deep learning engineers, and AI engineers. For this course, a basic knowledge of Python and Pytorch, and an awareness of machine learning in neural networks would be an advantage, though not strictly required. After completing this course, you will be able to apply positional encoding and detention mechanisms in transformer-based architectures, to process sequential data. Use and implement decoder-based models such as GPT, and encoder-based models such as BERT for language modeling, and implement a transformer model to translate text from one language to another. In Module 1 you'll learn about positional encoding and its implementation in Pytorch. You'll also learn how the attention mechanism works in language translation, how self-attention mechanisms help in language modeling, and how transformer-based models are used for text classification. Further, youll understand the functionality and implementation of the scaled dot product attention mechanism and also learn to enhance the efficiency of attention mechanisms. In the hands-on lab exercises, you'll use Pytorch in the Jupyter environment to implement a basic self-attention mechanism and positional encoding. You'll also apply transformers to perform text classification using a data loader. In Module 2 you'll learn about decoder models like GPT and encoder models like BERT. Along with their training and Pytorch implementation, you'll also learn to pre-train BeRT model using masked language modeling or MLM, and next sentence prediction or NSP. And also perform data preparation for BERT. Finally, you learn about the applications of transformers for translation by understanding the transformer architecture and its implementation. In hands-on labs you'll build and train a decoder GPT-like model, and an encoder model that is BERT model. Finally, you'll also construct a transformer model for language translation from scratch using Pytorch. This course has the right mix of content to facilitate learning. Videos are short and focused on the main topic. Readings provide detailed content primarily in text format. The labs provide the technical environment, detailed instructions and code snippets that you can use to complete hands on exercises. The practice and graded quizzes will help you apply what you learned and assess your knowledge. To get the most from your course, watch all videos, complete the labs to practice your new skills, and attempt all quizzes. Let's get started on this exciting journey. Good luck.