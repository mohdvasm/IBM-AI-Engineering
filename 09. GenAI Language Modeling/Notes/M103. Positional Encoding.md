Welcome to positional encoding. After watching this video, you'll be able to explain the importance of positional encoding, describe the techniques used to achieve positional encoding, and implement positional encoding in PyTorch. Have you ever tried unscrambling letters to form words? The order of position of the letters in a word is important to convey the intended meaning. Similarly, in transformer-based models, it's crucial to capture the position of each token to maintain the semantic meaning, as tokens are processed independently and simultaneously. Let's take an example. Consider the phrases King and Queen are awesome and Queen and King are awesome. These sentences are slightly different. Let's look at the embeddings for these. You'll see that the vector representation of embeddings is identical. You can introduce positional encoding to address this issue. It incorporates information about the position of each embedding within the sequence. Typically, positional encoding is added to the input embeddings, enabling the model to differentiate between the positions of various elements in the input sequence. You can see that after adding positional encoding, the vector representations of king and queen in the second sentence shown in green is different from their vector representations in the first sentence shown in blue. Positional encoding consists of a series of sine and cosine waves and involves two parameters. Let's look at them. The pos parameter represents the position of the sine wave over time, similar to the time variable t or the x coordinate in a standard plot. The parameter I, which is the dimension index, effectively generates a unique sine or cosine wave for each embedding, controlling the number of oscillations for each wave. Each one of these waves is added to a different dimension in the word embedding. Let's explore this further with an example. Let's begin with the sequence of embeddings for transformers are awesome, where each row represents an embedding for a specific word and each column corresponds to an element within that embedding. POS is the specific position of each word embedding within the sequence. In this example, the sequence length is three. Each word embedding has a dimensionality of four and will be differentiated based on whether its indices are odd or even. Let's add positional encoding to the embedding for transformers. For each dimension in positional embeddings, you introduce the corresponding sine and cosine waves. Thus, for i=0, a sine wave is added, and this pattern is repeated for i=1, ensuring that each dimension is uniquely represented by its own wave. You can view the values in a table. Similarly, the positional encoding values are calculated for the word are. Finally, the positional encoding values are calculated for the word awesome. You can view a table depicting the values for positional encoding corresponding to the input sentence. The four graphs illustrate positional encoding, one for each dimension of the word embeddings. For a sequence length of 3, you see only three values for each sine function.
Play video starting at :3:10 and follow transcript3:10
Lets generate positional encoding with an embedding dimension of 8 to depict a realistic setting. Occasionally, for practicality, you can align the maximum sequence size to match the vocabulary size by rotating the positional encodings. Rows become indicative of varying encoding functions, while columns represent time or position in a sequence. Positional encodings provide unique and periodic values for sequence positions. The cosine waves will never intersect the line at the same points, ensuring models distinguish and process variable length sequences effectively. Their range between minus 1 and 1 prevents overshadowing the embeddings. These encodings also support differentiability and relative positioning, making it easier to train the model. You can also represent each word embedding as a column vector x. Positional encoding can be conceptualized as a series of vectors, where each vector p captures a specific location within a sequence. When this positional vector is added to its corresponding embedding vector, the combination preserves the positional information, ensuring the elements sequence order is maintained within the resulting vector. In models such as GPT, positional encodings are not static but rather learnable parameters w. These learnable parameters, represented by tensors, are added to the embedding vector and optimized during training. Segment embeddings used in certain models, such as BERT, are related to positional encodings, providing additional positional information. You can integrate segment embeddings into the existing embeddings alongside the positional encodings. Let's now look at how positional encoding is implemented using PyTorch. Consider the embedding my_embeddings.
Play video starting at :4:52 and follow transcript4:52
You can construct a module to integrate positional encoding into the embeddings. The output shape sequence length is set to exceed the longest sequence in the training dataset, with the embedding dimension defining its depth. Next, you incorporate the token's positional encoding into the embeddings and apply dropout as a regularization technique to mitigate overfitting. You can apply positional encoding to get the encoded tokens.
Play video starting at :5:18 and follow transcript5:18
You define a module with learnable parameters for positional encoding. An nn.parameter tensor is instantiated with the desired shape. This positional encoding is then integrated with the embeddings. You can also apply dropout as a regularization strategy to reduce the risk of overfitting. Finally, the module applies these positional encodings to obtain the final encoded tokens.
Play video starting at :5:42 and follow transcript5:42
Let's now recap what you learned. In this video you learned that positional encoding incorporates information about the position of each embedding within the sequence. Positional encoding consists of a series of sine and cosine waves and involves two parameters. The pos parameter represents the position of the sine wave over time. The parameter I, the dimension index, effectively generates a unique sine or cosine wave for each embedding, controlling the number of oscillations for each wave. Positional encodings can also be learnable parameters. These learnable parameters, represented by tensors, are added to the embedding vector and optimized during training. Segment embeddings used in certain models, such as BERT, are related to positional encodings, providing additional positional information.