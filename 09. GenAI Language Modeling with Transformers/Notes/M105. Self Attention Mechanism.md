[MUSIC] Welcome to the self-attention mechanism. After watching this video, you'll be able to describe simple language modeling sequence and context embedding in the self-attention mechanism. You'll also be able to explain how self-attention mechanisms help in simple language modeling to predict the token. This self-attention mechanism is the heart of the language transformer. Each word in sequence attends every other word in parallel to generate embeddings. Lets begin by understanding simple language modeling in the self-attention mechanism. The simple language modeling in the self-attention mechanism predicts the next word in the sentence, an important aspect of understanding natural language. For example, in the table the sequence of words is presented on the left side and the prediction of words for the simple language modeling is on the right. Now, when you insert the word not like, the simple language modeling will predict the word hate. Similarly, the input word do like would predict the word like. It means that when the context of the word changes, the meaning of the preceding words also changes. The words transform into matrices of the embedding sequence where each embedded word represents a column vector within the sequence matrix. For example, X represents the matrix and the subscript not like represents the word sequence with each column corresponding to a word embedding. It means the matrix X not like contains the embedding for not and like. Therefore, each sequence is converted into a matrix representation and represented as a sample sequence in the dataset. Consider n sample sequences denoted as Xn. It means that each sequence can have a different length. It should be noted that libraries like Pytorch may arrange these vectors differently. Self-attention mechanisms and simple language modeling involve generating three key components, query Q, key K and value V. This process begins with the input word embeddings combined with learnable parameters. You can derive a query matrix by multiplying the input sequence matrix by learnable parameters known as query projection weights and biases. Here, a row vector of one s with the same length as the number of tokens is also used. Similarly, you can derive the key matrix by multiplying the input sequence matrix with a different set of learnable parameters, key projection weights and biases. You could apply the same process to obtain the value matrix where the input is again multiplied by its corresponding set of learnable parameters, known as value projection weights and biases. However, a bias term is not always included, and sequential embeddings are usually added before these operations. They are omitted in this simplified explanation for clarity. The self-attention mechanism employs the query key and value constructs to refine the input word embeddings. In this context, the square root of D represents the dimension of the embeddings. The resulting matrix, denoted as H', features columns that embody enhanced embeddings and are termed contextual embeddings. Further, the number of these embeddings aligns with the length of the input sequence, ensuring a one-to-one correspondence between the original and enhanced representations. You can apply an additional layer to the context embeddings where you will get output as H, a refined version of H' using additional learnable parameters. Therefore, H is a more nuanced representation of the initial embeddings. Self-attention mechanisms are instrumental in generating sequence outputs. For example, reducing the context embedding mechanism to an averaging operation helps to simplify the process. In this simplified framework, the contextual embedding serves as inputs to a neural network where the initial layers output is denoted as Z superscript 1. Lets consider a previous example of a self-attention mechanism used for translation, where the output of the self-attention mechanism is the embedded word you wish to translate. In this process, a dot product is computed between the embedding and a list of embeddings through matrix multiplication. The argmax function assists in obtaining the index of the translated words token. When using self-attention, the dot product is replaced by a neural network where Z1 represents the average embedding, serving as an input parameter for the neural network. This network generates a series of logits denoted as Z2. Moreover, the input dimension of the neural network aligns with the embedding space and its output dimension matches the vocabulary size. However, during training, the neural network employs the softmax function to compute the probability distribution across all potential words. This process helps to train the network and the learnable parameters of the attention mechanism. In Pytorch, logits are used directly during training and the argmax function is used to determine the sequences subsequent token or word index. Now lets predict the token for not like using the self-attention mechanism where the expected outcome is hate. The self-attention mechanism generates the query key and value matrices through multiplications with the learnable parameters and the bias. In this example, the input x is multiplied by the projections to obtain Q, K and V. This process consists of a series of matrix multiplications. These operations can be parallelized and processed via a graphic processing unit or GPU, allowing more data to be processed efficiently. This is one reason why the self-attention mechanism outperforms other sequential models like recurrent neural networks or RNNs. Additionally, you can obtain the attention scores by multiplying the query and key matrices, then apply the softmax function of these scores to normalize them. Next, multiply the normalized scores with the value matrix V to derive the H', which enhances the contextual embeddings by being multiplied with output projection parameters. Further, take an average of these embeddings and pass through the final layer that maps to the vocabulary, acquiring the activations corresponding to each word. Finally, apply the argmax function to these activations and the index obtained reveals the logistic value associated with hate. Further, diving into the attention mechanism, you can observe that each sequence is arranged with the input tokens positioned above, followed by the normalized attention, and the corresponding values beneath. These values act as modified word embeddings. For example, looking at the first two samples on the left, you can find the first column consistently represents the value for the token not, a pattern that holds true for each token. Meanwhile, the normalized attention underscores the unique relational dynamics between the pairs, highlighting the intricate interplay within the sequence. In this example, the attention scores derived from the dot product developments in keys and queries plays a critical role in the self-attention mechanism, shedding light on the models focal points. The attention scores are illustrated for three phrases, not like, not hate, and an arbitrary sequence elucidating the relationships between tokens within the sequence. Youve reached the end of this video. Lets recap, in this video, you've learned that simple language modeling in the self-attention mechanism predicts the next word in the sentence, an important aspect of understanding natural language. The words transform into matrices in the embedding sequence, where each embedded word represents a column vector within the sequence matrix. You also learned that the self-attention mechanisms in language modeling generate query key and value using the input word embeddings and learnable parameters. The self-attention mechanisms use the query, key and value in context embedding to modify the input row vectors. Finally, you learn that the self-attention mechanisms help generate output using various methods. [MUSIC]