Welcome to Encoder Models with BERT, pretraining using NSP. After watching this video, you'll be able to perform pre-training of BERT models using next sentence prediction or NSP tasks. You'll also be able to describe how BERT models can be fine- tuned for downstream tasks, such as question answering or sentiment extraction. BERT is pre-trained via self- supervised learning on vast text corpora, using mass language modeling or MLM and next sentence prediction or NSP. To perform the NSP task, the BERT model is trained to determine whether a given sequence logically follows a preceding one. Take for example, the sentence, My dog is cute. You would anticipate the following sentence to logically be, He likes playing. During BERT's training, a random sentence such as He likes studying medicine may be presented as a potential successor. The model's job is to predict which sentence is the appropriate continuation. You must alter the embeddings for this task. In BERT, you can perform tokenization using a standard tokenizer for simplicity and ease of coding from scratch. Please note that actual BERT uses word piece tokenization. Begin by adding the CLS token at the start of your sequence. This special token has a specific function. Following this, you will insert the SEP token to denote the end of a sentence. Each token will be associated with its respective embedding. Additionally, you can introduce segment embeddings, which serve to distinguish whether a token belongs to the first or the second sentence in paired sentence tasks. Finally, incorporate positional encodings to give the model an idea of the order of tokens in the sequence. Let's look at the segment embeddings. You have tokenized the sentences My dog is cute and He likes playing. For the segment embeddings, you need to assign a value of one to all tokens up to and including the first SEP token, indicating they belong to the first sentence. After the SEP token, assign a value of two, signifying that the subsequent tokens are part of the second sentence. Additionally, use a binary variable y to indicate sentence succession. A zero denotes that the sentence is not the logical follow up or NotNext. While one signifies that it is indeed the subsequent or next sentence. Let's set y=1. Here, insert a random sentence. My dog is cute, and He likes studying medicine. For the segment embeddings, follow the same procedure. But the binary variable, y=0, denotes that the sentence is not the logical follow up or NotNext. Let's consider an example of a data set. My name is Dave, followed by, I live in Toronto, is labeled as a consecutive sentence with the value set to one. This indicates that the sentences are sequentially related. Conversely, My name is Joyce. Toronto is the Capital of Ontario is labeled with a value set to false, indicating that the sentences are not related. In addition to the CLS and SEP tokens, incorporate zero padding to ensure all sentences are of uniform length for processing purposes. The labels have been padded for the masked language modeling as well, shown here as BERT labels. In the NSP task, the input consists of word embeddings that are processed by the encoder to generate contextual embeddings. These embeddings are used to determine whether for a given pair of sentences, the second sentence logically follows the first, just like a standard classification task. The CLS token highlighted in yellow corresponds to the NSP classification token, which encapsulates information from the entire sequence. This token is then fed into a neural network designed specifically for the NSP task, which assesses the relationship between pairs of sentences. Let's approach this as a two-class classification problem. Use the output value from the logits to determine the class. To train the encoder component of the BERT model, you can utilize the outputs from a neural network that has been trained for NSP and MLM tasks. The training process involves minimizing the combined loss, which is the sum of the losses from the two tasks. Accurate word prediction for masked words and the ability to discern whether the predicted word fits within the context. In PyTorch, the logits are used as inputs to the loss function to drive this optimization. After pre-training, you can fine- tune BERT for specific downstream tasks. Fine-tuning involves training BERT on a task-specific dataset, such as sentiment analysis, where the model predicts whether the sentiment of the input sequence is positive, neutral, or negative as shown. During fine-tuning, the BERT learns task-specific patterns and adapts its representations to perform well on the target task. Token representation of the CLS token will be transformed via a neural network to generate the model prediction. The contextual embeddings can also be used in many other tasks like vector databases. Let's now recap what you learn. In this video, you learned that to perform the next sentence prediction or NSP task, the model is trained to determine whether a given sequence logically follows a preceding one. The BERT model's job is to predict which sentence is the appropriate continuation. The CLS token is added at the start of the sequence, and the SEP token is added to denote the end of a sentence. Segment embedding serve to distinguish whether a token belongs to the first or the second sentence in paired sentence tasks. Positional encodings give the model an idea of the order of tokens in the sequence. In the NSP task, the input consists of word embeddings processed by the encoder to generate contextual embeddings. These embeddings are used to determine whether for a given pair of sentences, the second sentence logically follows the first, just like a standard classification task. Training the encoder component of the BERT model involves minimizing the combined loss, which is the sum of the losses from the two tasks. Accurate word prediction for masked words and the ability to discern whether the predicted word fits within the context. Fine-tuning involves training BERT on a task-specific dataset, such as sentiment analysis, where the model predicts whether the sentiment of the input sequence is positive, negative, or neutral.