Welcome to Transformer Architecture for Translation: PyTorch Implementation. After watching this video, you will be able to create an encoder decoder model for translation tasks using PyTorch and then train the model to generate German to English translations. To begin with, let's load the dataset using a batch size of 100, a hyperparameter that can be tuned during the training process. The output from the dataset loading process consists of pairs of sentences in German and English, representing the source and target languages for the translation task. These are denoted as src for the source, German sentences, and tgt for the target, English sentences. Just like training a decoder model, it's important to ensure that the model isn't influenced by future words for the target, so you should generate a causal mask. The image here shows the functions composition. The create_mask function is designed to construct masks for the source and target sequences within a transformer model. First, invoke the square mask from before. For the source sequence, the mask is typically an array of Boolean values initialized to false. There's no need to mask future tokens since the entire source sequence should be visible at once during the encoding process. Finally, this function also produces padding masks for both the target and source sequences. These masks flag the positions of padding tokens, commonly marked with zeros in the input tensors as true. Here, you have a positional encoding module that injects some information about the relative or absolute position of the tokens in the sequence. This is a token-embedding module for generating text embeddings in the form of a vector. Let's now put all the components together to create an encoder-decoder model for translation. First, create the embedding layers for the source and target. Next, create the positional encoding layer, then create the transformer. Finally, create a generator layer similar to the output layer on a neural network. There are several methods in this class. In the forward method, you'll first generate the token embeddings and positional encodings for both the source and target sequences. These embeddings are then provided as input to the transformer model, which comprises both the encoder and decoder components. Simultaneously, you'll apply the appropriate masks instructing the transformer to selectively ignore certain parts of the input sequence during processing. Finally, the output from the transformer passes through a linear layer, which is not shown here. This layer is responsible for generating the output vectors, which are the predictions returned by the model. Now, let's define the encoder method. It's important to note that this method is created for learning purposes only. In practice, the transformer layer itself handles both encoding and decoding processes. Within the encoder method, the source sequence undergoes token embedding and positional encoding. Subsequently, the sequence is processed by the actual encoder, resulting in an encoded vector, often referred to as the memory. Next, you'll define the decoder method, which takes the target sequence and the memory as inputs. The target sequence receives token embedding and positional encoding, similar to the source sequence. This prepared sequence is then passed to the decoder along with the memory, culminating in the generation of the result vector. The training process for transformers has some similarities to other methods. But let's focus on the key distinctions. Iterate over the source, src, and target, tgt, sequences provided in each batch. The target input is essentially the sequence with the last token removed. Then generate the necessary masks. A forward pass through the model generates the predictions. During this phase, you'll also input the target input. This procedure is similar to what happens during inference or prediction, where the transformer relies on its own previous outputs to make future predictions. The target output is essentially the target input shifted forward in time. Internally, the transformer employs the mask to recursively make predictions, building upon its previous outputs. The loss is computed using the target output and the output logits. Similarly, you can evaluate the loss on the training data using the validation data. The decoding process or inference begins by preparing the inputs. You'll start by constructing the memory tensor, which is derived by passing the source input and its corresponding mask through the model.encode function. Here, the German source text is encoded as illustrated. Next, initialize a tensor named ys, setting its dimensions to 1, 1 and filling it with the start symbol. This symbol serves as the initial token for the decoding process. Now, let's concentrate on the actual decoded text. In the decoder, you'll input the mask, ensuring attention is given only to the tokens already predicted. Alongside, pass the memory tensor which contains the encoded context from the source input. The output of the decoder behaves similarly to that of a neural network. Identify the maximum value corresponding to the predicted words index in the probability distribution generated by the model. This index is then used to retrieve the predicted word from the vocabulary. Now, add the output token index and show the output token here. This is used as input to the decoder. For the next iteration, repeat the process of getting the output token index. This is used as input to the decoder. For the next iteration, you'll repeat the process. You'll stop if you see the EOS tag or when you reach maximum iterations. As you can see here, the hyperparameters and other essential settings for the training process have been meticulously configured. Now, let's train the model. First, initialize the data loaders, train data loader, and validation data loader. Next, the transformer model is instantiated with parameters. For optimization, you can choose the Adam optimizer while defining learning rates and momentums. Then train the model using the train and evaluate the function. Here, you have a function called translate. It takes your model and a source sentence and gives back the translated sentence. In the loop, you can draw sentences from your dataset and print the original German sentence alongside its ground truth English translation and the model's translation. Since the primitive model was used, you can see that the model translation is very similar to the actual English translation. Let's now recap what you learned. In this video, you learned that the create_mask function is designed to construct masks for the source and target sequences within a transformer model. The linear layer is responsible for generating the output vectors, which are the predictions returned by the model. In practice, the transformer layer handles both encoding and decoding processes. Within the encoder method, the source sequence undergoes token embedding and positional encoding. Subsequently, the sequence is processed by the actual encoder, resulting in an encoded vector, often referred to as the memory. The decoder method takes the target sequence and the memory as inputs. The target sequence receives token embedding and positional encoding similar to the source sequence. This prepared sequence is then passed to the decoder along with the memory, culminating in the generation of the result vector. The translate function takes your model and a source sentence and gives back the translated sentence.