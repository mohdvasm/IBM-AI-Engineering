Welcome to pretraining BERT models with PyTorch. After watching this video, you will be able to describe how to create data loaders in PyTorch for BERT specific training tasks. You'll also be able to explain the steps to creating a BERT model and PyTorch and describe how to train the BERT model using PyTorch. First, create a custom dataset class called BERTCSVDataset to load data from the CSV file into a PyTorch dataset. The get item method retrieves an individual item from the dataset based on the index IDX. It accesses the corresponding row from the data frame and converts the JSON formatted strings back to tensors, including BERT input, BERT label, segment label, and is next. The collate function, collate batch is used for batching and padding sequences in a PyTorch data loader. Now you can create data loaders for the train and test datasets using PyTorches data loader class. You start by defining the batch size. Next, you specify the file paths for the train and test datasets. You can then create instances of the BERTCSVDataset class for the train and test datasets using the specified file paths. Finally, create the corresponding data loaders. The BERT embedding class is a module used for embedding operations and positional encodings in a BERT model. The main difference from other models is that you add segment embeddings. Let's now define the complete BERT model. Define a BERT class, a subclass of torch.nn.Module. The init method initializes the BERT model by specifying the vocabulary size, model dimension, d_model, number of layers, n_layers, number of attention heads, heads, and dropout rate, dropout. The BERT model includes an embedding layer that combines token embeddings and segment embeddings. This is achieved with the BERT embedding class defined earlier. You can use transformer encoder layers to encode the input embeddings. The number of transformer layers in the specified configurations are determined by the n_layers, heads, dropout, and d_model parameters. The model has a linear layer for next sentence prediction or NSP, which predicts the relationship between two consecutive sentences. It takes the output from the transformer encoder with the shape of sequence length, batch size, embedding dimension. The input is the CLS embedding for each sequence. Similarly, a linear layer for masked language modeling or MLM predicts the mask tokens in the input sequence. It takes the output from the transformer encoder and produces predictions across the vocabulary. The expected shape of the output is sequence length, batch size, vocab size. The forward method defines the forward pass of the BERT model. It takes bert_inputs, the input tokens, and segment_labels as inputs and returns the predictions for NSP and MLM. Let's now see how to set up the parameters and create an instance of the BERT model. The embedding dimension is 10, and the vocabulary size is determined based on the length of vocab. This mini BERT model has two transformer layers and an initial number of attention heads set to two. The drop out rate is 0.1. With these parameters, you can create the BERT model instance for further use. Now that the model is created, the next step is to train it on the data and assess its performance. To facilitate this process, you can define and evaluate function. This function will be used to evaluate the BERT model's performance and to assess the model's progress during the training phase. First, the loss function is defined. The loss function is cross entropy loss, which you can use to calculate the loss between the predicted and the actual values. The evaluate function takes several arguments, data loader, model, loss_fn, and device. Within the function, the BERT model is put into evaluation mode using model.eval (). This disables dropout and other training specific behaviors. Variables are initialized to keep track of the total _loss, total _next_sentence loss, total_mask_loss and total number of batches. The evaluation is performed with gradients turned off using torch.no_grad (). This saves memory and computation since gradients are not needed for validation. The function iterates through the batches in the provided data loader. A forward pass is performed with the BERT model to obtain predictions for the next sentence and mask language tasks. The two losses are then calculated and summed up to obtain the total loss for the batch. You can calculate the average loss, average next sentence loss, and average mask loss by dividing the total losses by the total number of batches. The average losses are printed and returned from the function. Before the training starts, define the optimizer for training the BERT model. The optimizer is Adam. Let's look at the training loop. Within each epoch, you can iterate through the training data and batches. For each batch, perform the forward pass, calculate the loss, and update the models parameters through back propagation and gradient clipping. After each epoch, you can print the average training loss and evaluate the model's performance on the test set. You can also save the model after each epoch. Now, let's check how the models perform. For this to find a function called predict NSP, which predicts whether a second sentence follows the first sentence using a pretrained BERT model. The function takes two sentences, a BERT model and a tokenizer as inputs. The function tokenizes the input sentences using the tokenizer.encode_plus method, which returns a dictionary of tokenized inputs. The tokenized inputs are then converted into tensors and move to the appropriate device for processing. The BERT model is used to make predictions by passing the token and segment tensors as input. The first element of the logics tensor is selected and unsqueeze to add an extra dimension, making its shape 1, 2. The logets are passed through a softmax function to obtain probabilities, and the prediction is obtained by taking the argmax. Finally, the prediction is interpreted and returned as a string, indicating whether or not the second sentence follows the first. For example, two sentences are passed to the predict_nsp function along with a BERT model and tokenizer. The result is printed indicating whether the second sentence follows the first or not based on the models prediction. Next, you can define a function to perform MLM, using the pretrained BERT model. The function takes a sentence, a BERT model, and a tokenizer as inputs. The function tokenizes the input sentence using the tokenizer and converts it into token IDs, including the special tokens. The result is stored in the token tensor variable. Dummy segment labels filled with zeros are created as segment labels. The BERT model is used to make predictions by passing the token tensor and segment labels as input. The MLM logits is extracted as predictions. The position of the mask token is identified using the nonzero method and stored in the mask token index variable. Remember that all tokens accept the mask token are zero padded.The predicted index for the mask token is obtained by taking the argmax of the MLM logits at the corresponding position, which is of the shape, batch size, sequence length, vocab size. The predicted index is converted back to a token using the convert_ids_to_tokens, method of the tokenizer. The original sentence is then replaced with the predicted token at the position of the mask token, resulting in the predicted sentence. The predict_mlm function is called with an example sentence, a BERT model and a tokenizer. The predicted sentence is printed with the mask token replaced by the predicted token. Let's now recap what you learned. In this video, you learned that to create a data loader, you will create a custom dataset class to load data from a CSV file into a PyTorch dataset. Use a collate function and create corresponding data loaders for the train and test datasets. To create a BERT model, you'll define a class for embedding operations and positional encodings, and a BERT model class, both subclasses of torch.nn.module. In the BERT model class, you will define the embedding layer, transformer encoder layers, linear layers for NSP and MLM, and the forward pass of the BERT model. As part of BERT model training, you'll define a function to evaluate the BERT model's performance. Define the optimizer for training the BERT model, define the training loop, and view the loss.