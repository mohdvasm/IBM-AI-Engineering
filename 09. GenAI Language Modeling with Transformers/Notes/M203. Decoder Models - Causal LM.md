Welcome to the video Decoder Models' PyTorch Implementation Causal Language Model. After watching this video, you'll be able to describe how to create a decoder model for the next token prediction with a causal language model or LM using PyTorch. You'll also be able to explain how to collate functions and mask future tokens using a causal mask. Additionally, you'll also be able to explore the role of Custom GPT architecture in the decoder model. Decoder models leverage just the decoder of a transformer model. At every stage for a specific word, the attention layers can only access the words placed before it in the sentence. Such models are usually termed as autoregressive models. However, the pre-training of decoder models typically incorporates the prediction of the next word in the given sentence. Therefore, these models are most useful in text generation. The decoder models are similar to the generative pre-trained transformer or GPT. Let's begin by understanding how to create a decoder model for future text tokens using PyTorch and causal LM. Internet Movie Database or IMDB test and validation splits, are useful for training and evaluating the decoder model. Each data record includes a sentiment label, often ignored, and a textural element is prioritized to create training data. However, customized vocabulary is useful for tokenizing and mapping the textual data to indices. To achieve this, first load the IMDB dataset. In neural language processing, or NLP, certain special tokens, such as unknown tokens, UNK, padding tokens, pad, and end-of-sentence tokens, EOS are used. These tokens serve widely recognized purposes, such as an unknown token, UNK, represents words not found in the vocabulary, typically during inference. However, the padding token, PAD, equalizes the lengths of sequences in a batch for models that require inputs of the same size. The end-of-sentence token, EOS, signifies the end of a sequence, which is crucial for models to understand when a sentence or sequence of tokens ends. Next, language models predict words by analyzing the previous text, where context length acts as a hyperparameter. Let's look at the process visualization steps. First, specify context size, which means block size and input text. Next, randomly select a point in sequence from the text of length block size and shift the source by one token to generate a target sequence. This function helps to check several parameters such as block size, text, random sample stop, random start, and source sequence. Let's invoke the function "get_sample" using the block size of 10. You can observe the source and target sequences and verify that both sequences are 10 tokens long. Based on your verification, you've observed that one token shifted the source forward. Next, use the collate function in parallel to collate the previous functions. The function "get_sample" retrieves the source analysis of the previous text, and helps target using block sizes as a global variable. It means that padding helps to ensure the sequences are of the same size in a batch. Next, compare the source and target indices to reveal that the tokens shift sequentially. Now, let's understand how to create a causal mask to prevent the model from being influenced by future words. First, use the function, "generate_square_subsequent_mask". This function helps to form a diagonal matrix, transposes it, and set zero values to negative infinity. However, PyTorch includes the function "generates_square_subsequent_mask" to create causal masks with sequence length as the input. You can see that the outcome of this function is an upper triangular matrix filled with negative infinities, which are dimmentioned equally in rows and columns to the sequence length. PyTorch creates a causal mask for transformers, ensuring predictions for a token that are dependent on the previously visible tokens. They also use a function such as "generate_square_subsequent_mask" and create_mask to complete this task. Further, the encoder ignores the zero padding using a padding mask, where padding tokens are converted to true. Now, let's explore the role of Custom GPT model architecture in creating a decoder model to predict the next token. The Custom GPT model consists of an embedding layer, a positional encoding layer, a transformer encoder, and a linear layer that is lm_head for language modeling. First, the embedding layer maps each input token to a dense vector of size embed_size. However, the positional encoding layer adds positional information to the input embeddings. Further, the transformer encoder consists of multiple layers of multi-head self attention, adding a source mask. This helps the encoder to behave like a decoder. Finally, the linear layer outputs the logits over the vocabulary size.
Play video starting at :4:44 and follow transcript4:44
Next, the forward method of Custom GPT model takes as an input sequence of tokens x, and adds the positional encoding. Further, add the source mask, SRC mask, and padding mask to the encoder to get the output as a sequence of contextual embeddings. This output is converted to logits. Now you've reached the end of this video. Let's recap. In this video, you've learned to create a decoder model to predict the next token. Each data record includes a sentiment label and a textual element to create training data. To predict the decoder model for the next token, you need to specify the context size and randomly select a point in the sequence. You also learned that the collate function can be used in parallel to collate the previous functions. To create the causal mask, first, use the function "generate_square_subsequent_mask". To form a diagonal matrix, transpose it, and set the zero value to negative infinity. Finally, you learned that the Custom GPT model architecture helps create a decoder model to predict the next token. Here, the embedding layer maps each input token to a dense vector of size embedded size, and the linear layer outputs the logits over the vocabulary size.