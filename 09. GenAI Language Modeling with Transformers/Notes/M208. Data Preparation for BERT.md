Welcome to the Data Preparation for BERT with PyTorch video. After watching this video, you will be able to prepare data for pre-processing using tokenization and masking. You will also create training-ready input for BERT's specific training tasks such as MLM and NSP. For data preparation, the initial steps involve tokenization and constructing a vocabulary. You can follow the standard PyTorch pipeline with two nodes. First, initialize the tokenizer using the get tokenizer function. In this case, the tokenizer is set to tokenize text using the basic English model. Please note that actual BERT uses word piece tokenization. Following that, you'll define special symbols and their corresponding indices; PAD IDX, CLS IDX, SEP IDX, MASK IDX, and UNK IDX. PAD is used to represent padding tokens. CLS represents the start of a sequence. SEP is used as a separator between sequences. MASK is used for masked language modeling, or MLM, and UNK represents unknown tokens. The masking function is used to perform masking operations on tokens. It takes a token as input and decides whether to mask it. If the mask is false, the function immediately returns the original token with a pad label. If the mask is true, the function randomly selects an operation with a 50% chance for each option. In case 1, the token is replaced with mask, and the label is set to a random token from the vocabulary. In case 2, the token remains unchanged, and the label is set to the same token. In case 3, the token is replaced with mask and the label is set to the original token. The function then returns the modified token and its label. The prepare for MLM function is used to prepare tokenized text for BERT's MLM training. The function takes a list of tokens as input and initializes lists to store the process sentences, labels for each token, and raw tokens if needed. It applies BERT's MLM masking strategy to each token, replacing it with a mask token and assigning a label indicating whether the token was masked or left unchanged. The function checks if the token is a sentence delimiter. If a valid sentence is found with more than two tokens, it adds the current sentence and its labels to the respective lists. If raw tokens are included, the original tokens are stored as well. The function continues this process until all tokens are processed. Any remaining tokens are added as a sentence if present. Finally, the function returns the prepared lists for BERT's MLM training. Let's check a sample MLM output. Each token in a sentence is labeled depending on the masking operation that's applied to that token. In this example, the first the is masked. Therefore, BERT input is mask and its BERT label is the. Tokens, sun, sets, behind, and the last the are not changed, so their corresponding labels are pad. Distant is masked and replaced with a random token. Therefore, BERT input is mask and its BERT label is human scaled. Finally, mountains and period are unchanged, so their corresponding labels are pad. The process for NSP function is used to prepare data for the next sentence prediction, or NSP, task in BERT training. This function takes a list of tokenized sentences and a corresponding list of masked labels as inputs. It verifies that both lists have the same length and enough sentences. The function initializes lists to store the sentence pairs for BERT input. The masked labels for the sentence pairs and binary labels indicating whether the sentences are consecutive or not. Using a while loop, the function generates sentence pairs until there are at least two available indices remaining. It randomly selects whether to create a next sentence scenario or a not next sentence scenario. For the next sentence scenario, it selects two consecutive sentences, appends them with CLS and SEP tokens, and assigns the corresponding labels and a one as the is next label. For the not next sentence scenario, it selects two random distinct sentences, appends them with CLS and SEP tokens and assigns the corresponding labels and a zero as the is next label. After processing each pair, the used indices are removed from the available indices list. Finally, the function returns the prepared lists for BERT's NSP task, the sentence pairs, the masked labels, and the is next labels. Let's check a sample output. Special symbols, CLS and SEP, are first added to the input sentences. BERT label is created using the prepare for MLM function. In the first example, the second sentence follows the first sentence. Therefore, is next is one. In the second example, the second sentence does not follow the first sentence, so is next is zero. The prepare BERT final inputs function is used to prepare the final input lists for BERT training. It takes the BERT inputs, BERT labels, and is next lists as inputs, and pads are reformatted, reshaped, and converted into a tensor. Inside the function, there is a nested function called zero-pad list pair, that is used to zero-pad the input pairs with pad tokens to ensure they have the same length. This is important for maintaining consistent input shapes during training. The function also includes Lambda functions, flatten and tokens to index for flattening nested lists and transforming tokens to their corresponding vocabulary indices respectively. The function iterates over each input pair, creates segment labels for each pair of sentences, and apply zero padding to the input pairs and segment labels using the zero-pad list pair function. In BERT, segment embeddings are used to differentiate between the two input sentences. This is necessary because BERT's input consists of a pair of sentences for NSP or question answering type tasks, and the model needs to understand which tokens belong to which sentence. If the to tensor flag is set to true, the function flattens the padded inputs and labels, transforms the tokens to indices, and converts them to PyTorch sensors. Otherwise, if to tensor is false, the flattened padded inputs and labels, segment labels and is next values are appended directly to the final output lists. Let's check a sample output. Sentences are zero-padded, and each token is mapped to its vocab index; CLS to one, he to 33, SEP to two, pad to zero. Mask labels are also padded and mapped to vocab indices. In this case, all tokens are unchanged except the token he, which is masked. Next, segment labels are created, where tokens of the first sentence are labeled with one, tokens of the second sentence are labeled with two, and zero paddings are labeled with zero. Finally, is next is returned to indicate that the second sentence does not follow the first one. This code writes the processed Internet Movie Database, or IMDB, data to a CSV file. The output will be as shown here. Let's check an example of the outputs of data preparations. Given a corpus, pairs of sentences are created as input. Input is tokenized and numericalized. Special tokens, CLS and SEP, are added. Then it'll be zero-padded. Next, the masking strategy is applied. According to the masking strategy, BERT labels are created where every token has a label zero, except for mask tokens. Segment labels are also created that show to which sentence each token belongs. Finally, the is next label shows whether the second sentence follows the first one. Let's now recap what you learned. In this video, you learned that for data preparation, first, you must initialize a tokenizer using get tokenizer function and then define special symbols with their corresponding indices. The masking function is used to perform masking operations on tokens, where it takes a token as input and decides whether to mask it. The prepare for MLM function takes a list of tokens as input and initializes lists to store the process sentences, labels for each token, and raw tokens, if needed. It applies BERT's MLM masking strategy to each token, replacing it with a mask token and assigning a label indicating whether the token was masked or left unchanged. The process for NSP function takes a list of tokenized sentences and a corresponding list of masked labels as inputs. It verifies that both lists have the same length and enough sentences. It initializes lists to store the sentence pairs for BERT input. The masked labels for the sentence pairs and binary labels indicating whether the sentences are consecutive or not. The prepare BERT final inputs function is used to prepare the final input lists for BERT training. It takes the BERT inputs, BERT labels, and is next lists as inputs.