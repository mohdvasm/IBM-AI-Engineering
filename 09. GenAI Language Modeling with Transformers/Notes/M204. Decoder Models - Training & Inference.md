Welcome to decoder models, PyTorch implementation using training and inference. After watching this video, you'll be able to create small instances of the models and calculate the loss. You'll also be able to describe the models training process and explain how to generate text auto regressively. The decoder model in PyTorch refers to a neural network architecture that uses encoded information and generates output sequences. Decoder models help in sequence-to-sequence text generation tasks, such as machine translation and image captioning. It's important to note that the decoder model is similar to the generative pretrained transformer or GPT. Let's begin by understanding how to create a small instance of the model. Create a small instance of the model, first specify the model hyperparameters, including encoder layers of two and the number of attention heads of two. Then define the instance for the custom GPT model. Now let's calculate the loss. During loss calculation, the encoder model generates a source and a target. However, examining a batch from the target, you can observe that the dimensions are sequence length by batch size. During prediction, the decoder model generates logits, Class 1 and Class 2. It's important to note that the dimensions of the logits are sequence length, batch size, and number of classes. Additionally, each table represents one sample of a batch. In preparation for loss calculation, you can see the reshaping of logits where each row corresponds to the prediction for a token, spanning across both the sequence and the batch dimensions. Next to calculate the loss, you can reshape the target tensor so that its elements correspond correctly to the logits. This process ensures that every row from the logits aligns with the appropriate target outcomes for accurate loss estimation. The illustration represents the Omega hat, which is useful for its simplicity, and all the related logits are in a real world scenario. However, the training process is similar to other models such as convolutional neural networks or CNNs, recurrent neural networks or RNNs, transformers and generative models. It uses the modified loss shape and other functions such as validation and checkpoint saving that help in the optimization. The evaluate function measures the performance of the model by computing its average loss in the validation dataset. However, the trained model is useful to generate inferences. Now, let's understand how to generate auto regressive text, also known as inference. Autoregressive text generation is a method of generating text where the model predicts the next token in a sequence based on the previous tokens. This process is iterative, where the decoder model generates one token at a time and uses the generated token as input for the next prediction. Preparing an encoding prompt helps to create a process for text generation. This process serves as a starting point for the model to generate subsequent tokens. Once this prompt is tokenized, the decoder model can process and generate the next tokens based on the input. Here, the pad tokens are used to process multiple batches. However, they're not necessary for a single prediction. For example, a tokenized decoded prompt is mapped with vocab indices and padding. The generate function creates auto regressive text in the decoder model. To do so, first, create the encoded prompt tensor from the input sentence, complete the sentence width. Then feed the prompt into the model. You can see that the mask is generated in the model, and the model will output the logits during the process. Next, select the final logit vector. Now, identify and generate the token with the highest logit score, in this case, the. Further, append the, to the prompt for subsequent auto regressive text generation. You can see that the decoder model receives the updated prompt, and generates a token with the highest logit value. In this case, the token is next. Now append a token next to the prompt to generate the next round of auto regressive text. You can see that the decoder model has again received the updated prompt, and generated a token with the highest logit value. In this case, it is a word. Now append the token word to the prompt to generate the next round of autoregressive text. Finally, this loop continues until the token reaches the other end of the sentence, or passes max new tokens to the decoder model. Now you've reached the end of this video. Let's recap. In this video, you've learned to create small instances of the models by specifying the model hyperparameters. The encoder model generates a source and a target while calculating loss. The training process is similar to other models and uses the modified loss shape and other functions that help in optimization. Autoregressive text generation is a method of generating text where the model predicts the next token in a sequence, based on the previous tokens. You learned the function generate to create an autoregressive text in the decoder model. Finally, you learned to generate an auto regressive text, create an encoded prompt, and feed it into the model. Then generate a mask and you'll see the logit output and select it. Next, identify and generate the token with the highest logit score and append it for generating subsequent autoregressive text.