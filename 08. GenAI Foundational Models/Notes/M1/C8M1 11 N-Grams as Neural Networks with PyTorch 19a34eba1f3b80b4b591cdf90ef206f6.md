# C8M1.11. N-Grams as Neural Networks with PyTorch

Last edited: February 15, 2025 1:57 AM
Tags: Course 08

Welcome to N-grams as neural networks with PyTorch. After watching this video, you will be able to create an n-gram model in PyTorch and train it. You will also be able to use n-grams as neural networks with PyTorch. In PyTorch, you can create an embedding layer with an arbitrary vocabulary size and set the embedding dimension and a context size of two. The next layer's input dimension must be the product of this context vector embedding dimension and the context size. Start with two indices representing the input context size of two. These two samples are used as input for the embedding layer. The output is two embedding vectors of dimension 3. Use the method reshape to reshape the embeddings to form the context vector, which is the context size or number of samples multiplied by the embedding dimension. This concatenates all the samples together. This is used as an input to the next layer. In PyTorch, the n-gram language model is essentially a classification model using the context vector and an extra hidden layer to enhance performance. The n-gram model predicts words surrounding a target by incrementally shifting what's known as a sliding window. In a bigram model, the prediction for words at position t is based on the positions t-1 and t-2, beginning at t = 3 to preclude negative indices. Each table row represents a distinct value that aligns with the words position. Column 2 displays the context and column 3 shows the predicted word, both dependent on row t. Consider the phrase, I like vacations, with each word's index beneath it. For t = 3, the context is I like shown in blue and the predicted target word is vacations in red. Moving to t = 4, presented in row 2, the context updates to like vacations with and next prediction. This method is applied throughout the entire sequence. In PyTorch for n-gram modeling, implement windowing to create batches of context and target words. It initiates the process where targets and contexts are generated using a batch function. A for loop iterates from the start of the context size, which in this example begins at two marking vacation as your initial target within the window. I is the target index. The context is gathered by subtracting j from, then slide the window forward, incrementing i to capture successive targets and contexts for the entire sequence. Here, you are going to create a toy data set. Now, you'll create a pipeline that converts the text to indexes. Instead of using a dataset object, you will use a list object. In training the model, prioritize the loss over accuracy as your key performance indicator. Train the model similarly to your classification model by padding tokens to ensure consistent shape. Here, you will pad with previous values for alignment. The index_to_token attribute obtained via vocab.get_itos is a list where each element corresponds to a word and the index within the list corresponds to that word's token index. This list serves as a mapping to translate the numerical output of a neural network, which could represent class or token indices back into a human readable format, essentially acting as a decoder. Thus, when a neural network predicts an index, this list can be used to retrieve the associated word. Let's make a prediction. Apply a text processing pipeline to the string never gonna. The result is the list 3, 1, which represents the token indices. Convert the list of token indices into a PyTorch tensor. Make a prediction with the model and select the index with the largest value. Finally, convert the index to a token using the index_to_token mapping, you can use this function to generate a sequence of words using a model. Let's now recap what you learned. In this video, you learned that an n-gram model allows for an arbitrary context size. In PyTorch, the n-gram language model is essentially a classification model using the context vector and an extra hidden layer to enhance performance. The n-gram model predicts words surrounding a target by incrementally shifting it as a sliding window. In training, the model, prioritize the loss over accuracy as your key performance indicator or KPI.