# C8M1.6. Document Categorization Training with Torchtext

Last edited: February 15, 2025 1:52 AM
Tags: Course 08

Welcome to document categorization training with torchtext. After watching this video, you'll be able to explain what cross-entropy loss is and describe how optimization reduces losses in your model. A neural network functions via matrix and vector operations called learnable parameters. Here, you'll identify and obtain these parameters. With networks having millions to trillions of parameters, Theta is used to collectively represent them. In neural network training, learnable parameters are fine tuned to enhance model performance. This process is steered by the loss function, which serves as a measure of accuracy. The goal is to find the optimal parameter values denoted as Theta that minimize the discrepancy between the model's predicted output hat y and the actual label y. As your focus is solely on the parameters, you'll consider the loss function explicitly as a function of Theta. Although the term loss and costs are often used interchangeably, for clarity, the terminology consistent with PyTorch's framework is followed. The table compares actual labels y and predicted labels y hat from a neural network, featuring vectors x. Correct predictions are in green and errors are in red, showing 20% accuracy. The aim is to adjust Theta to lower loss and improve accuracy. As the parameter Theta is adjusted, you'll observe a reduction in the loss, leading to an alignment between the true class labels y and the predicted labels hat y shown by the increase in green. Concurrently, there's an increase in the model's accuracy, which is a positive indicator of performance improvement. Now you'll learn how cross-entropy is used to find the best parameters. Recall that you input an embedding vector into the network. For each category, the neural network outputs a vector of logits, where each logit is a score reflecting the article's likelihood of fitting a particular news category as shown here. The associated table links vector elements to categories. World is first followed by sports, business, and science and technology. Logits are transformed into probabilities through the softmax function depicted here. This process, given input x, computes the probability that the predicted class hat y corresponds to a certain category. Each logit is exponentiated to ensure positivity and then normalized against the sum of all logits. This creates a conditional distribution for each predicted class. That is, given a sample x, the probability of belonging to the class world, sports, business, and science and technology is shown here. The softmax transformation plays a crucial role in enhancing the distinction between scores assigned to different classes. The true probability distribution y is shown here. Each bar represents the probability of y equals 0, 1, 2, 3. Let's begin by evaluating the expected value difference between the true distribution, that is the probability of y, and the conditional distribution, that is the probability of y given x and Theta. Rather than simply calculating the difference and squaring it, apply the logarithm to both distributions before finding their difference known as the KL divergence. Only the second term, which is referred to as cross-entropy loss, is dependent on Theta. The problem now is finding the distribution of y, for which you can employ a useful technique. You can compute the expected value of a function using the distribution, the probability of z, depicted here. However, if the distribution is unknown, you can estimate it by averaging the function applied to a set of samples. This technique is known as Monte Carlo sampling. Monte Carlo sampling can be applied to approximate the true cross-entropy loss by averaging the output of a function over samples x and y as shown here. You can calculate the conditional distribution using the softmax function. It should be noted that usually the batches of data have been used, but not all the samples. In PyTorch, the loss function utilizes the network's output logit z, which is a proxy for y hat and the true labels y for computation. In PyTorch, after importing the cross-entropy loss first, define a text classification model and then create a cross-entropy loss object. The model calculates logits for a given input, then it computes the loss by comparing these predictions to the true labels using PyTorch cross-entropy loss function. Now you are going to do optimization, which is the method used to minimize the loss. The gradient descent equation is key to reducing loss in the model. Here's the breakdown. To update your next parameter to Theta_k + 1 from Theta_k using the current parameter values nudging closer to minimal loss, the learning rate Eta sets your step size. The gradient of the loss function points where the loss changes most. By moving in the reverse direction of this gradient, you can fine tune your parameters, thereby decreasing the loss with each step. Now let's observe the process unfold. Gradient descent starts with an initial random parameter guess denoted as k equals zero. In the first iteration, that is k equals one, adjust parameters using the gradient of the loss function scaled by a learning rate of 0.1 and add this to the parameters from k equals zero to compute the loss. This process is repeated for k equals two, updating the parameters and recalculating the loss, which decreases incrementally. This iterative refinement continues, reducing loss and enhancing model accuracy with each step. This image depicts a 2D loss function surface with a chosen starting point. The algorithm fine tunes the parameters iteratively toward the minimum loss visualized by the red point moving downward along the white trajectory line that converges on the lowest loss point. In practical use, neural networks have millions of parameters, leading to a complex loss surface as shown here. Loss can vary sharply, rising or falling quickly. Many strategies exist to reduce loss, known as optimization methods. These strategies will not be detailed here, but will be referred to as methods to improve optimization or training. Typically, you partition your data set into three subsets: training data for learning, validation data for hyperparameter tuning, and test data to evaluate real-world performance. In this case, you have predetermined the hyperparameters, and now your focus must be on test data. In PyTorch, you will first initialize the stochastic gradient descent or SGD optimizer for the model's parameters with a learning rate, or LR, of 0.1. A scheduler reduces the learning rate by a factor of Gamma after each epoch, which enhances the optimization process. Reset all gradients for the variables to be updated, that is the learnable weights of the model, to zero before the next iteration. The model makes a prediction and calculates the loss. The backward function computes the derivatives of the loss. Finally, gradient clipping is applied to improve optimization. Let's now recap what you learned. In this video, you learned that a neural network functions via matrix and vector operations called learnable parameters. In neural network training, learnable parameters are fine tuned to enhance model performance. This process is steered by the loss function, which serves as a measure of accuracy. The prediction function works on real text that starts by taking in tokenized text. It processes the text through the pipeline and the model predicts the category. Cross-entropy is used to find the best parameters. For unknown distribution, estimate it by averaging the function applied to a set of samples. This technique is known as Monte Carlo sampling. Optimization is used to minimize the loss. Generally, the data set should be partitioned into three subsets: training data for learning, validation data for hyperparameter tuning, and test data to evaluate real-world performance.