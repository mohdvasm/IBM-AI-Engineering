# C8M1.4. Converting words into features

Last edited: February 15, 2025 1:49 AM
Tags: Course 08

Welcome to converting words to features. After watching this video, you'll be able to describe one-hot encoding and bag of words. You'll also be able to explain embeddings and embedding bags and describe how to use embedding and embedding bag and PyTorch. Consider you're developing a Natural Language processing or NLP application to classify email. You can categorize email s in various ways based on the presence of specific words, the frequency of words, or the contextual meaning of words in the email. To implement this functionality, it's essential to convert words to numerical features so that machine learning models can process the text. Let's explore the details. Consider the following sample sentences. I like cats. I hate dogs. I'm impartial to hippos. Sets of sentences, documents, or sequences are often termed a corpus or data set. Let's use a neural network to identify the subject cat, dog, or hippo. The input needs to be translated into numbers the neural network can understand. One-hot encoding is a method used to convert categorical data into feature vectors that a neural network can understand. Consider a table with the first column representing the token index and the second column representing the token. The third column represents the one-hot encoded vector. The dimension of the vector corresponds to the number of words in the vocabulary and the token index represents the element of each vector. The token I is represented with all elements set to zero except for the one corresponding to I. This will be the feature x for the word I. The token like is represented in a similar way, you can observe a pattern. This will be the feature x for the word like. The token cats is represented in a similar way and so on. You can represent each token in a document as a vector. How do you use these vectors to depict the entire document or sequence? The bag of words representation portrays a document as the aggregate or average of one-hot encoded vectors. For I like cats, you combine the one-hot vectors, you add the one-hot vectors for I like and cats. The x vector is the bag of words vector for cats. Let's look at embedding and embedding bags. Consider a neural network used to classify the previous sentences about cats, dogs or hippos. You input the bag of words vector, and select the output value with the highest association for cats. You'll understand how the embedding and embedding bag layers can effectively replace this initial linear layer. Let's look at the hidden layer when we input the one-hot vector for the word cat, which has zeros everywhere except the seventh position, the activation in the hidden layer corresponds to the parameters of the seventh neuron.

Play video starting at :2:38 and follow transcript2:38

Instead of using a one-hot encoded vector, you can substitute it with the token index. In this instance seven, the output mirrors the one-hot vector. The layer that accepts this index is called the embedding layer, and its output is the embedding vector. The embedding weights are combined to form an embedding matrix. The number of columns is the embedding dimension. Each row represents a word. Embedding vectors generally have a lower dimensionality compared to one-hot encoded vectors. Reducing the dimensionality simplifies the computational requirements for the model. When you feed a bag of words vector to a neural network's hidden layer, the output is just the sum of the embeddings.

Play video starting at :3:20 and follow transcript3:20

It also helps to view embedding as a row in the embedding matrix, labeling them with the word they represent. This process entails summing up all the vectors from bag of words and multiplying the result with the embedding matrix.

Play video starting at :3:36 and follow transcript3:36

Instead of doing this, you can use an embedding bag layer. The input is simply the indexes for each token and the output is the sum of word embeddings. Let's now look at how to use embeddings and embedding bag in PyTorch. First, you need the tokens. Initialize the tokenizer iterator from the dataset and vocabulary. The input_ids function tokenizes and generates the indexes for each data sample. These indexes are stored in the list index where each element in the list is the indexes for the different documents. You can initiate the embedding layer and specify the dimension size of the embeddings. Next, determine the count of unique tokens present in the vocabulary. Now, create the embedding layer embeds using the nn.embedding constructor. You apply the embedding object. The input is the index for the phrase I like cats to retrieve the embedding. What you'll see is a PyTorch tensor representation. Each row in this tensor aligns with the respective embedding for the words I like and cats. You can retrieve the embedding for the last data sample, I'm impartial to hippos using its index. The result is a PyTorch tensor. Each row corresponds to the embedding for each tensor word. Initializing the embedding bag layer is almost the same as the embedding layer. Let's output the embedding bag for the first document using its index, you can access the embedding for I like cats. The outcome is a PyTorch tensor representing the sum of average of all embeddings. For one sample, the offset parameter is always zero. Let's explore the offset parameter. In NLP datasets are often represented as one dimensional tensors. However, when working with embedding bags and other applications, it's crucial to identify the position of each document. Let's dive into this with our sample dataset. You combine the tensors of individual documents using the Cat function with the index setting. The offset parameter captures the starting position of every document. You can count the tokens in each sample. This step helps in pinpointing the initial position. Using the cumulative sum method, you can accumulate the lengths, thereby determining the starting position of each sequence. You use the embedding bag function along with the offset parameter as with the index tensor. The result is the embedding bag. For each individual document you have the average of its words, embeddings. Let's now recap what you learn. In this video you learned about one-hot encoding, bag of words, embedding and embedding bags. One-hot encoding converts categorical data into feature vectors. The bag of words representation portrays a document as the aggregate or average, of one-hot encoded vectors. When you feed a bag of words vector to a neural network's hidden layer, the output is the sum of the embeddings. The embedding and embedding bag classes are used to implement embedding and embedding bags in PyTorch.