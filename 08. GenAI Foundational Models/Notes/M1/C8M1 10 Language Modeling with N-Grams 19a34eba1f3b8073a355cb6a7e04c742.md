# C8M1.10. Language Modeling with N-Grams

Last edited: February 15, 2025 1:56 AM
Tags: Course 08

Welcome to language modeling with n-grams. After watching this video, you will be able to explain the concepts of bi-gram and tri-gram models. You will also be able to describe and illustrate language modeling with n-grams. Let's begin with understanding about n-grams. Consider the phrases I like and I hate. How would you complete these sentences? For instance, if you were given I like, followed by a blank, would you choose surgery or vacation? Most people would likely complete it with vacation upon seeing the word like. Similarly, with the phrase I hate followed by a blank, the expected completion is surgery, reflecting common associations with these words. These examples demonstrate how your word choices are often influenced by the context provided by preceding words. Consider the phrase I like vacations, which can be represented as a sequence. You can depict this using a table with each column labeled omega 1 to 3 representing a word and the subscript indicating its position in the sentence. That is, word 1, word 2, and word 3. The same goes for I hate surgery. This is where the bi-gram model comes into play. It's a conditional probability model. What's the probability of your third word, word three, given the second one, word two, word two represents your context, the second word, which is crucial in guiding the prediction. In a bi-gram model, your context size is one, meaning you'll consider only the immediate previous word to predict the next one. Given the second word is like, that is, word two equals like, what could the third word be? Using the table, count the occurrences of each potential follow up to like in the third column. By examining the table, observe that surgery never follows like. Hence, the probability of word three being surgery is zero. However, vacation appears after like, and the probability of word three being vacation is one. Consider the phrases surgeons like surgery and I hate surgery. Adding surgeons changes the context of the sentence, making it at least plausible that surgeons like surgery. Let's create the same table. The tri-gram model is also a conditional probability function and can improve on the bi-gram model's limitations by increasing the context size to two. Now, in addition to word two, you also use word one to predict word three. Let's examine the probability of word three equaling surgery. Given that word one is I and word two is like, you will find that the probability is zero. However, if you change word one to surgeons, the probability of word three being surgery is now one. Consider the phrases surgeons like surgery, I hate surgery, and I like vacations. How would you complete the phrase surgeons like? You can employ the tri-gram model to predict the next word by leveraging the probabilities generated from the table. For context, set the first word to surgeons and the second to like to predict the third word, denoted as omega. Identify which word maximizes the likelihood of being the third word. This optimization is achieved using the argmax function, which selects the word with the highest probability from the different possible third words, given that the first word is surgeons and the second is like. As illustrated, all possible outcomes for the third word have a probability of zero, except for surgery. Therefore, the predicted third word is surgery. Let's consider predicting the next word, word 3, given that the first word is I and the second word is like. If instead you're aiming to predict a word at some arbitrary point t, using the preceding points t1 and t2. And if you're assuming these relationships remain constant over time, that is, implying stationarity, then you can apply the tri-gram or bi-gram models at any point in time. For the tri-gram case, you can use words at positions t2 and t1 to predict the word at position t. Following the same predictive process, you can generalize the concept of a tri-gram to an n-gram model, which allows for an arbitrary context size. In this expanded framework, t denotes the chosen context size. However, calculating the probabilities for larger context sizes can become increasingly complex. Fortunately, neural networks offer a solution by approximating these probabilities. Specifically, apply the softmax function to estimate the probabilities, and then train a neural network to predict the next word based on the previous words. The context word is used as a feature, that is, the context vector to define the previous words. Let's revisit the previous example where each word in your vocabulary of six words, that is, I, hate, like, surgeons, surgery, vacations, is represented by a six dimensional, one hot encoded vector. Consider encoding the phrase I like with a context size of two. If you employed a bag of words model, the vector representation of I like would be identical to that of like I. To establish the concatenate vector, concentrate the individual vectors corresponding to each word, as illustrated here. Instead of performing the operation on raw one hot vectors, concatenate the one hot encoded embedding vectors. In the realm of neural networks, the context vector is generally defined as the product of your context size and the size of your vocabulary. Typically, this vector is not computed directly, instead, you should construct it by concatenating the embedding vectors. Let's examine the neural network architecture for an n-gram model. Imagine you're working with a small problem, a six-word vocabulary and a two word context. Recall the neural classifier designed to categorize one of four articles which has four output neurons. However, the network must predict one of six possible outputs, one for each word in the vocabulary. So there are six neurons in the output layer. Given the context size of two, adjust the input accordingly. The input dimension is the product of vocabulary size and context size. With six words and a context size of 2, your input dimension is 12. Utilize the final layer's output directly without needing to convert it to probabilities. This feedforward neural network ignores the dependence on t, as it does not have a built-in mechanism to capture the order or position of words in a sentence, like recent neural networks that work better at generating text. Let's now recap what you learned. In this video, you learned that a bi-gram model is a conditional probability model with context size one. That is, you consider only the immediate previous word to predict the next one. A tri-gram model is also a conditional probability function and can improve on the bi-gram model's limitations by increasing the context size to two. The concept of a tri-gram can be generalized to an n-gram model, which allows for an arbitrary context size. In the realm of neural networks, the context vector is generally defined as the product of your context size and the size of your vocabulary. Typically, this vector is not computed directly, but is constructed by the embedding vectors close.