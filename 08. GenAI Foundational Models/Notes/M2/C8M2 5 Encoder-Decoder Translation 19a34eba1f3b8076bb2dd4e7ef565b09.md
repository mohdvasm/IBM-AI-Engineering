# C8M2.5. Encoder-Decoder Translation

Last edited: February 15, 2025 2:10 AM
Tags: Course 08

Welcome to encoder decoder RNN models translation, after watching this video you will be able to implement an encoder decoder RNN model. Let's learn about the encoder decoder architecture. As you know, RNNs can be used to create sequence to sequence models that receive sequence x as input and generate sequence y as output. Note that x and y do not necessarily need to be of the same length. For this purpose, encoder decoder architectures are introduced. A popular example of a sequence to sequence model is a translation model. Let's check an actual example to see how encoder decoder RNNs generate translation. The encoder decoder sequence to sequence model works together to transform an input sequence into an output sequence. Encoder is a series of RNNs that process the input sequence, individually passing their hidden states to their next RNN. The last hidden state context is passed to the decoder module. The decoder module similarly is a series of RNNs that autoregressively generates the translation as one token at a time. Each generated token goes back into the next RNN along with the hidden state to generate the next token of the output sequence until the end token is generated. As RNNs become more complex, you can abstract away the finer details. Here, an RNN is represented with an RNN cell. The key distinction here is that the output from the previous state is recycled as input for the next state. For the encoder, the interest lies only in the hidden state and to not use the output. This is because only the decoder will generate the output text. The encoder is only responsible for encoding the input sequence. Now that you have an understanding of the overall architecture, let's delve into the internals of the encoder units. The encoder consists of multiple encoder units, each responsible for processing input tokens. At the heart of each unit is an embedding layer which transforms the input token into an embedded vector. These vectors then traverse through the RNN cell to produce the hidden state, which is subsequently handed over to the RNN cell of the next encoder unit. To build an encoder in PyTorch, define a class inheriting from torch.nn.module similar to a standard neural network. The embedding vectors serve as input to a long short term memory or LSTM layer characterized by n layers, that is, the number of recurrent layers and hid dim, that is the size of the hidden and cell states. Incorporate dropout as an optimization technique to enhance performance, in the LSTM layer input embeddings are transformed into hidden states and cell states, with hid dim specifying the number of neurons. During the forward method execution, the embedding layer processes the input and the LSTM layer outputs the hidden and cell states. Only these states are retained. The output vector is discarded since it's not required by the encoder. Remember, unlike gated recurrent units or GRUs, which have only hidden states, LSTMs maintain an additional cell state for each layer. The decoder is also composed of many decoder units within which the predicted token is generated. Each unit has an embedding layer, where embedded vectors are created. These vectors then pass through the RNN cell to output the updated hidden state, which will be passed to the RNN cell of the subsequent decoder unit. Next, a linear layer maps the RNN output to the output dimension to generate the next token. As you can see, the decoder works a bit differently from the encoder in the way that it receives its previously generated token and generates the next token autoregressively. To build a decoder in PyTorch, create a decoder class, which is a subclass of nn module serving as the base class for neural network modules. In the constructor init method, the decoder's parameters and layers are initialized. The parameters for the decoder are defined as follows. Output dim is the number of possible output values or the target vocabulary length. M dim is the dimensionality of the embedding layer. Hid dim is the dimensionality of the hidden state in the LSTM. N layers is the number of layers in the LSTM. Dropout is the dropout probability. The decoder includes an embedding layer that maps the output values to dense vectors of size m dim. An lstm layer that takes the embedded input and produces hidden states of size hid dim. Next, a linear layer maps the LSTM output to the output dimension output dim. Finally, a soft max activation function is applied to the output, generating a probability distribution over the output values. To construct the sequence to sequence model, define the sequence to sequence class inheriting from nn module with the encoder decoder device and target vocabulary trg vocab as inputs. The forward method accepting source and target sequences and teacher forcing ratio manages the model's forward pass. Teacher forcing is a training technique used in sequence to sequence models that involves using the true or correct output sequence from the training data as input to the decoder. Instead of using the previously generated output from the model itself, it boosts model training. Initialize batch size trg len and trg vocab size create output tensor to hold the decoder's predictions at each step. Extract hidden and cell states from the encoder of using the encoder src and set them as initial states for the decoder. Start the decoder with the first target sequence token. For each target sequence time step, pass the input and prior states to the decoder, saving the output tensor and outputs. Decide whether you need to apply teacher forcing at each step using the true target token trg T or the predicted one output argmax 1, depending on the teacher forcing ratio. Then return the output's tensor which includes predictions for each time step. Completing the sequence to sequence model construction, let's now recap what you learned. In this video you learned that RNNs can be used to create sequence to sequence models that receive one sequence as input, and generate another sequence as output. The encoder decoder architectures are introduced so that the sequences do not necessarily require to be of the same length. The encoder decoder sequence to sequence model works together to transform an input sequence into an output sequence. Encoder is a series of RNNs that process the input sequence, individually passing their hidden states to their next RNN. The last hidden state context is passed to the decoder module. The decoder module similarly, is a series of RNNs that autoregressively generate creates the translation as one token at a time. The embedding layer transforms the input token into an embedded vector, which traverses through the RNN cell to produce the hidden state.