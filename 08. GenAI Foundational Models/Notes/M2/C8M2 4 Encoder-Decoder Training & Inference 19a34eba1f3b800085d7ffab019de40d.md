# C8M2.4. Encoder-Decoder Training & Inference

Last edited: February 15, 2025 2:09 AM
Tags: Course 08

Welcome to encoder decoder RNN models, Training and Inference. After watching this video, you will be able to use a translation dataset in PyTorch to train and infer an encoder decoder RNN based model. You'll be working with the Multi 30K dataset which includes training validation and test sets of English to German text. Before delving into implementing the sequence to sequence or seek to seek model, let's review how to load data in batches for training models using PyTorch's data loader. [A.py](http://a.py/) file has been created that fetches the multi 30 K dataset. The collation, that is tokenization, numericalization, and addition of beginning of sequence or BOS, end of sequence or EOS, and padding have also been done. You will also see the created iterable batches of SRC and TRG tensors. As a first step, you must download and run [the.py](http://the.py/) file, then call the get translation data loaders function with an arbitrary batch size to create the data loaders for training and validation. You can also check a data sample. In general, sequence to sequence models are more difficult to train than RNNs and several factors come into play. However, typically the aim is to minimize the cross entropy loss by summoning the output of the predicted outcomes with the actual labels. Let's use blue to denote a correct prediction. The procedure for training sequence to sequence models has almost similar steps, but let's highlight similar differences from neural networks. First, initialize the model in training mode to activate essential layers like dropout, ensuring optimal performance during training. Iterate through training data batches, assign input SRC and target TRG sequences to the correct device, and generate predictions by output. For sequence models like RNNs where the input shape often differs from other model types, it's crucial to reshape the outputs tensor too here. Here, target length 1 represents the rows excluding the initial BOS or beginning of sequence token, ensuring you don't include it in loss computation. Batch size is the columns each representing a separate sequence in the batch and output dimension corresponds to the columns representing the predicted output for each token in the sequence. This reshaping aligns the rows and columns correctly for loss calculation, matching the output predictions to the target dimensions. Finally, you can calculate the average loss per batch after processing all batches, you will also create and evaluate function that is identical to the training function, but you must use different data and set the model to valve to speed it up. Let's see how to make a prediction. Performing translations in sequence models requires a more complex function for making predictions. The function takes a model, a source sentence, a target language vocabulary, and a maximum translation length. Firstly, convert the source sentence to the correct format, then feed it into the models encoder to obtain hidden and sell states. Start the target tensor with the BOS token to initialize the translation and reshape it. To generate the translation iterate up to Max Len. Utilize the last target tensor and the previous states in the decoder to obtain new outputs and states. Then choose the next token with the highest probability, add it to the translation and store it. End the process if the EOS token appears else feed the output to the input. Finally, convert token indices to words, remove special tokens, and concatenate the tokens to form the translated sentence. Now you're ready to run all the functions in the lab. Let's now recap what you learned in. In this video, you learned that in general, sequence to sequence models are more difficult to train than RNNs due to several factors. In model training, the aim is to minimize the cross entropy loss by summing the output of the predicted outcomes with the actual labels. The procedure for training sequence to sequence models includes initializing the model and training mode to activate essential layers like dropout, ensuring optimal performance during training, Iterating through training data batches, assigning input SRC and target TRG sequences to the correct device, and generating predictions by output, reshaping the outputs tensor, which aligns the rows and columns correctly for loss calculation. Finally, calculating the average loss per batch after processing all batches. Performing translations in sequence models requires complex functions for making predictions.