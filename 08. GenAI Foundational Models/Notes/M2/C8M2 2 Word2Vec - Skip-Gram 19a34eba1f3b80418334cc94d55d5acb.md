# C8M2.2. Word2Vec - Skip-Gram

Last edited: February 15, 2025 2:07 AM
Tags: Course 08

Welcome to Word2Vec: skip-gram and pretrained models. After watching this video, you will be able to describe Word2Vec skip-gram model. Create a skip-gram model in PyTorch and train it and explain large pretrained embedding models. A skip-gram model is a reverse of the Continuous Bag of Words or CBOW model. It predicts surrounding context words from a specific target word. For instance, it aims to forecast words t-1 and t+1 for a given target word, t. At t = 1, with exercises as the target, the model predicts she and every. At t = 2, when every is the target, it aims to predict exercises and day. The terminology may be confusing as the target is not the dependent variable. In this depiction, exercises is the selected target word encoded as a one hot vector within the vocabulary space, marked by a one for exercises and zeros for all other terms. The output layer is tasked with predicting the surrounding context words, she and every. Ideally, after training, the models output for each context position should have the highest logic values for the actual context words. The skip-gram model simplifies the task by predicting one context word at a time from a target word. For example, for t =1, with exercises as the target, it predicts the preceding context word she. Independently, it predicts the following context word every. This makes the complex context prediction easier by breaking it into smaller manageable tasks. Now let's examine the neural network. At the input, you have one hot encoded vector for exercises and prediction is she. The second pair is exercises and every. Let's take a look at the skip-gram model in PyTorch. To create a skip-gram model, first you will initialize it. Then you will define the embeddings layer using nn embedding, which creates word embeddings for the given vocabulary size and embedding dimension. Fc layer is a fully connected layer with an input dimension or embed dim and an output dimension of vocab_size. In the forward method, the input text is passed through the embeddings layer to obtain word embeddings. Next, you will apply activation and linear layers. Finally, you will create an instance of the skip-gram model. The sequence generation function is identical to CBOW, but with a switch in the order of the target and context. This breakdown allows you to work with the full context in smaller parts. The code segment breaks the context into smaller, manageable segments. Let's now look at the first sample in the code snippet. Each sample consists of two elements, the target word and its context. The nested loop iterates through the context, that is, the second element of the sample, and appends each part to the target. This results in the original full context being segmented into smaller discrete parts. Next, you will flatten the data. The collate function is similar to the CBOW model. You will then create a data loader object. Let's examine a single batch from the data loader you have created. The tensors are returned, including the target and context. You can also observe the indices. Additionally, the tokens are displayed here, which makes it quite intuitive. To train the parameters, you will define the learning rate and loss function CrossEntropyLoss criterion. The optimizer optimizes the parameters of the model which are obtained by model CBOW parameters. For skip-gram model, you only need to change the input object of the optimizer. Then you will define a learning rate scheduler. Now let's define the training function to train the model for a specified number of epochs. You must include a condition to check whether the input is for skip-gram or CBOW. The output of this function includes the trained model and a list of average losses for each epoch. Now to retrieve word embeddings, you will call the train function with the data loader you created for training the CBOW model. Once the model is trained, you can retrieve the weights which are the actual word embeddings. You can get the embedding vector for a specific word by its index. Now you will learn about text classification using pretrained word embeddings. Word embeddings utilize Stanford's pretrained GloVe or global vectors that leverage large scale data for word embeddings. Using a different method. It can be integrated into PyTorch via torchtext.vocab for improved NLP tasks such as classification. You will initialize GloVe with GloVe within parentheses, name equals within single quotes 6B, to load pre trained vectors and create a custom vocab object to match different tokens. You've developed a text classification model and integrated GloVe word embeddings into a PyTorch layer. You use dot from pretrained with GloVe vectors, 6B.vectors. You will create a word amending bag operation. You can also try setting freeze to false if you have a larger dataset. That's it. Check out the lab for more. Let's now recap what you learned in this video. You learned that skip-gram model predicts surrounding context words from a specific target word. It operates in contrast to the CBOW model. The skip-gram model simplifies the task by predicting one context word at a time from a target word. The sequence generation function in the creation of the skip-gram model is identical to CBOW, but with a switch in the order of the target and context. This breakdown allows you to work with the full context in smaller parts. While training the parameters, you define the learning rate, loss function, optimizer and learning rate scheduler. Once the model is trained, you can retrieve the weights, which are the actual word embeddings. Word embeddings utilize Stanford pretrained GloVe or global vectors that leverage large scale data for word embeddings.