# C8M2.1. Word2Vec - CBOW

Last edited: February 15, 2025 2:06 AM
Tags: Course 08

Welcome to Word2Vec, introduction and CBOW models. After watching this video, you'll be able to explain the concept of word2vec and describe word2vec's continuous bag of words, or CBOW model. You will also learn to create a CBOW model. So what is word2vec? It is the short form for word to vector. It is a group of models that produce word embeddings or vectors, which are numerical representations capturing the essence of words. For example, king is closer to man and queen to woman due to their similar meanings. Interestingly, subtracting man from king yields a vector similar to queen, illustrating the effectiveness of these embeddings in capturing word relationships. These vectors can be utilized in natural language processing, or NLP tasks to enhance performance by substituting the randomly generated embeddings. To acquire these vectors, you can begin with randomly generated embeddings. You can use a neural network model consisting of an input layer, an embedding layer, and an output layer. Words are fed into an embedding layer, which interacts with a softmax output layer to predict context words. Training involves tuning the W hidden layer and W prime output layer weights to refine word vector representations. The number of neurons in both input and output layers corresponds to the vocabulary size, while the embedding layer's size, which is chosen by the user, defines the word vector dimensions. After training the neural network, if the network predicts queen following woman with a higher probability than it does for man, the embedding for queen will be closer to woman than to man. Similarly, if king is probabilistically more associated with man than with woman, the resulting vector for king is closer to man than woman. Now, let's use the network to attempt to predict words from the given window using other words within the window. Set the window width to one. At t equals one, exercises is the target word with she and every as context words. At t equals two, every becomes the target surrounded by exercises and morning in context. For the context and target words with a window width of one, the context includes the word at t minus one and t plus one. The target word is at t. At t equals one, the context consists of she and every. The target word is exercises. As you move to t equals two, the context includes exercises, morning and the target word is every. Next, you will learn about the continuous bag of words, or CBOW model. This model utilizes context words to predict a target word and generate its embedding. In the table shown, the second column lists the predicted word from the network. When she and every are given as inputs, the model predicts exercises. With exercises and morning as inputs, the model predicts every. In the CBOW model for the sentence, she exercises every morning, the input dimension of four matches the number of unique words in the corpus. The goal is to predict word likelihood, so the output dimension is also four. The target word is exercises with context words, she and every combined into a bag of words vector. This vector passes through the hidden layer containing word embeddings. In the output layer, expect the highest logic value for exercises, indicating the model's prediction for this context. Now, let's shift the context window to consider new inputs. The words, exercises and morning are encoded as one hot vectors and provided as inputs to the model. During training, your goal should be to fine tune the model's weights so that it effectively predicts the target word every. Which should correspond to the highest logic value in the output layer. To create the CBOW model, first you will initialize it. Then you will define the embedding layer using nn embedding bag, which calculates the average of context word embeddings. You will also configure the fully connected layer using self.fc, with input size embed dim and output size vocab size. In the forward method, you will pass input text and offsets through the embedding layer, which retrieves context word embeddings and calculates their average. Next, you will apply the ReLu activation function. Afterward, the output of the ReLu activation goes through the fully connected layer. Finally, you will create an instance of the CBOW model. Next, you will initialize the tokenizer using steps one and two. Using step three, you will create a vocabulary from tokenized data. In step four, you will set the context size to two. Then you will slide over the text to form context target pairs. Next, using step five, you will set up a text processing pipeline, a data loader, and a batch function for a network. It will have a bag of words of batch size 64 to prepare batches for training on next word prediction tasks. Let's now recap what you learned. In this video you learned that word2vec is the short form for word to vector. It is a group of models that produce word embeddings, or vectors, which are numerical representations capturing the essence of words. A neural network model consists of an input layer, an embedding layer, and an output layer. Words are fed into an embedding layer, which interacts with an output layer to predict context words. The number of neurons in both input and output layers corresponds to the vocabulary size. While the embedding layer's size, which is chosen by the user, defines the word vector dimensions. The continuous bag of words, or CBOW model utilizes context words to predict a target word and generates its embedding. [MUSIC]